% Encoding: UTF-8

@Article{Agnew2000,
  author     = {J. Agnew and J. Thornton},
  title      = {Just noticeable and objectionable group delays in digital hearing aids.},
  pages      = {330--6},
  volume     = {11 6},
  file       = {:C\:/Users/aron/Google Drive/BA/thesis/literature/agnew2000.pdf:PDF},
  journal    = {Journal of the American Academy of Audiology},
  owner      = {Aron Petau},
  readstatus = {skimmed},
  timestamp  = {2021-05-14},
  year       = {2000},
}

@Book{american2013diagnostic,
  author    = {{American Psychiatric Association}},
  title     = {Diagnostic and Statistical Manual of Mental Disorders},
  doi       = {10.1176/appi.books.9780890425596},
  publisher = {American Psychiatric Association},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/american2013diagnostic.pdf:PDF},
  month     = may,
  timestamp = {2021-05-15},
  year      = {2013},
}

@Article{anderson2017plasticity,
  author    = {Carly A. Anderson and Diane S. Lazard and Douglas E.H. Hartley},
  title     = {Plasticity in bilateral superior temporal cortex: Effects of deafness and cochlear implantation on auditory and visual speech processing},
  doi       = {https://doi.org/10.1016/j.heares.2016.07.013},
  issn      = {0378-5955},
  note      = {Plasticity Following Hearing Loss and Deafness},
  pages     = {138--149},
  url       = {http://www.sciencedirect.com/science/article/pii/S0378595516301174},
  volume    = {343},
  abstract  = {While many individuals can benefit substantially from cochlear implantation, the ability to perceive and understand auditory speech with a cochlear implant (CI) remains highly variable amongst adult recipients. Importantly, auditory performance with a CI cannot be reliably predicted based solely on routinely obtained information regarding clinical characteristics of the CI candidate. This review argues that central factors, notably cortical function and plasticity, should also be considered as important contributors to the observed individual variability in CI outcome. Superior temporal cortex (STC), including auditory association areas, plays a crucial role in the processing of auditory and visual speech information. The current review considers evidence of cortical plasticity within bilateral STC, and how these effects may explain variability in CI outcome. Furthermore, evidence of audio-visual interactions in temporal and occipital cortices is examined, and relation to CI outcome is discussed. To date, longitudinal examination of changes in cortical function and plasticity over the period of rehabilitation with a CI has been restricted by methodological challenges. The application of functional near-infrared spectroscopy (fNIRS) in studying cortical function in CI users is becoming increasingly recognised as a potential solution to these problems. Here we suggest that fNIRS offers a powerful neuroimaging tool to elucidate the relationship between audio-visual interactions, cortical plasticity during deafness and following cochlear implantation, and individual variability in auditory performance with a CI.},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/anderson2017.pdf:PDF},
  journal   = {Hearing Research},
  keywords  = {Audio-visual interactions, Cortical plasticity, Functional near-infrared spectroscopy, Occipital cortex, Speechreading, Superior temporal cortex},
  timestamp = {2021-05-14},
  year      = {2017},
}

@Misc{audacity,
  author    = {Audacity},
  title     = {Audacity® software is copyright © 1999-2020 Audacity Team.Web site: https://audacityteam.org/. It is free softwaredistributed under the terms of the GNU General Public License.The name Audacity® is a registered trademark of Dominic Mazzoni.},
  groups    = {Materials},
  timestamp = {2021-05-14},
  year      = {2020},
}

@Article{badian1979standardized,
  author    = {Badian, M and Appel, E and Palm, D and Rupp, W and Sittig, W and Taeuber, K},
  journal   = {European journal of clinical pharmacology},
  title     = {Standardized mental stress in healthy volunteers induced by delayed auditory feedback (DAF)},
  year      = {1979},
  issn      = {0031-6970},
  number    = {3},
  pages     = {171--176},
  volume    = {16},
  doi       = {10.1007/bf00562057},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/badian1979.pdf:PDF},
  publisher = {Springer},
  timestamp = {2021-03-07},
}

@Misc{bates2014lme4,
  author        = {Douglas Bates and Martin Mächler and Ben Bolker and Steve Walker},
  title         = {Fitting Linear Mixed-Effects Models using lme4},
  eprint        = {1406.5823},
  archiveprefix = {arXiv},
  file          = {:bates2015lme4.pdf:PDF},
  primaryclass  = {stat.CO},
  timestamp     = {2021-05-14},
  year          = {2014},
}

@Article{beker2018autismDevelopment,
  author    = {Shlomit Beker and John J. Foxe and Sophie Molholm},
  title     = {Ripe for solution: Delayed development of multisensory processing in autism and its remediation},
  doi       = {https://doi.org/10.1016/j.neubiorev.2017.11.008},
  issn      = {0149-7634},
  pages     = {182--192},
  url       = {http://www.sciencedirect.com/science/article/pii/S0149763417305262},
  volume    = {84},
  abstract  = {Difficulty integrating inputs from different sensory sources is commonly reported in individuals with Autism Spectrum Disorder (ASD). Accumulating evidence consistently points to altered patterns of behavioral reactions and neural activity when individuals with ASD observe or act upon information arriving through multiple sensory systems. For example, impairments in the integration of seen and heard speech appear to be particularly acute, with obvious implications for interpersonal communication. Here, we explore the literature on multisensory processing in autism with a focus on developmental trajectories. While much remains to be understood, some consistent observations emerge. Broadly, sensory integration deficits are found in children with an ASD whereas these appear to be much ameliorated, or even fully recovered, in older teenagers and adults on the spectrum. This protracted delay in the development of multisensory processing raises the possibility of applying early intervention strategies focused on multisensory integration, to accelerate resolution of these functions. We also consider how dysfunctional cross-sensory oscillatory neural communication may be one key pathway to impaired multisensory processing in ASD.},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/beker2018.pdf:PDF},
  journal   = {Neuroscience & Biobehavioral Reviews},
  keywords  = {Autism spectrum disorder, Typical development, Multisensory integration, Sensory processing, Amelioration, Normalization, Time window of integration, Recovery, Oscillation, Phase alignment, Phase reset},
  timestamp = {2021-05-12},
  year      = {2018},
}

@Article{bertelson2003visual,
  author    = {Bertelson, Paul and Vroomen, Jean and De Gelder, B{\'e}atrice},
  title     = {Visual recalibration of auditory speech identification: a McGurk aftereffect},
  doi       = {10.1046/j.0956-7976.2003.psci_1470.x},
  issn      = {0956-7976},
  number    = {6},
  pages     = {592--597},
  subtitle  = {A McGurk Aftereffect},
  volume    = {14},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/bertelson2003recalibration.pdf:PDF},
  groups    = {Multisensory Integration},
  journal   = {Psychological Science},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  timestamp = {2021-05-14},
  year      = {2003},
}

@Article{biau2015speaker,
  author    = {Biau, Emmanuel and Torralba, Mireia and Fuentemilla, Lluis and de Diego Balaguer, Ruth and Soto-Faraco, Salvador},
  journal   = {Cortex},
  title     = {Speaker's hand gestures modulate speech perception through phase resetting of ongoing neural oscillations},
  year      = {2015},
  issn      = {0010-9452},
  pages     = {76--85},
  volume    = {68},
  doi       = {10.1016/j.cortex.2014.11.018},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/biau2015handGestures.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Elsevier},
  timestamp = {2020-11-30},
}

@Article{bosker2020visual,
  author    = {Bosker, Hans Rutger and Peeters, David and Holler, Judith},
  title     = {How visual cues to speech rate influence speech perception},
  doi       = {10.1177/1747021820914564},
  issn      = {1747-0218},
  pages     = {1747021820914564},
  volume    = {73},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Bosker2020_lexical selection_audiovisual.pdf:PDF},
  groups    = {Multisensory Integration},
  journal   = {Quarterly Journal of Experimental Psychology},
  publisher = {SAGE Publications Sage UK: London, England},
  timestamp = {2021-05-12},
  year      = {2020},
}

@Article{Bosker2021,
  author    = {Bosker, Hans Rutger and Peeters, David},
  journal   = {Proceedings of the Royal Society B: Biological Sciences},
  title     = {Beat gestures influence which speech sounds you hear},
  year      = {2021},
  issn      = {0962-8452},
  month     = jan,
  number    = {1943},
  pages     = {20202419},
  volume    = {288},
  abstract  = {Beat gestures—spontaneously produced biphasic movements of the hand—are among the most frequently encountered co-speech gestures in human communication. They are closely temporally aligned to the prosodic characteristics of the speech signal, typically occurring on lexically stressed syllables. Despite their prevalence across speakers of the world's languages, how beat gestures impact spoken word recognition is unclear. Can these simple ‘flicks of the hand' influence speech perception? Across a range of experiments, we demonstrate that beat gestures influence the explicit and implicit perception of lexical stress (e.g. distinguishing OBject from obJECT), and in turn can influence what vowels listeners hear. Thus, we provide converging evidence for a manual McGurk effect: relatively simple and widely occurring hand movements influence which speech sounds we hear.},
  doi       = {10.1098/rspb.2020.2419},
  eprint    = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspb.2020.2419},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/bosker2021.pdf:PDF},
  groups    = {Multisensory Integration},
  owner     = {Aron Petau},
  publisher = {The Royal Society},
  timestamp = {2021-03-09},
  url       = {https://royalsocietypublishing.org/doi/abs/10.1098/rspb.2020.2419},
}

@Article{Box1964,
  author       = {Box, G. E. P. and Cox, D. R.},
  date         = {1964},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  title        = {An Analysis of Transformations},
  doi          = {https://doi.org/10.1111/j.2517-6161.1964.tb00553.x},
  eprint       = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1964.tb00553.x},
  number       = {2},
  pages        = {211--243},
  url          = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1964.tb00553.x},
  volume       = {26},
  abstract     = {Summary In the analysis of data it is often assumed that observations y1, y2, …, yn are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters θ. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
  journal      = {Journal of the Royal Statistical Society: Series B (Methodological)},
  month        = jul,
  publisher    = {Wiley},
  year         = {1964},
}

@Article{Bramsloew2010,
  author    = {Bramsløw, Lars},
  journal   = {International journal of audiology},
  title     = {Preferred signal path delay and high-pass cut-off in open fittings},
  year      = {2010},
  issn      = {1499-2027},
  month     = sep,
  number    = {9},
  pages     = {634--44},
  volume    = {49},
  doi       = {10.3109/14992021003753482},
  file      = {:bramslow2010.pdf:PDF},
  publisher = {Informa {UK} Limited},
  timestamp = {2021-03-23},
}

@Article{brandwein2013development,
  author    = {Brandwein, Alice B and Foxe, John J and Butler, John S and Russo, Natalie N and Altschuler, Ted S and Gomes, Hilary and Molholm, Sophie},
  journal   = {Cerebral Cortex},
  title     = {The development of multisensory integration in high-functioning autism: high-density electrical mapping and psychophysical measures reveal impairments in the processing of audiovisual inputs},
  year      = {2013},
  issn      = {1053-8119},
  month     = apr,
  number    = {6},
  pages     = {1329--1341},
  volume    = {23},
  doi       = {10.1016/j.neuroimage.2013.12.029},
  file      = {:C\:/Users/Aron/Google Drive/BA/thesis/literature/brandwein2013highFunctioning.pdf:PDF},
  publisher = {Oxford University Press},
  timestamp = {2021-03-07},
}

@Article{bridges2020timing,
  author    = {Bridges, David and Pitiot, Alain and MacAskill, Michael R and Peirce, Jonathan W},
  journal   = {PeerJ},
  title     = {The timing mega-study: comparing a range of experiment generators, both lab-based and online},
  year      = {2020},
  month     = jan,
  pages     = {e9414},
  volume    = {8},
  doi       = {10.31234/osf.io/d6nu5},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/bridges2020timing.pdf:PDF},
  publisher = {PeerJ Inc.},
  timestamp = {2021-03-07},
}

@Article{calvert1997activation,
  author    = {Calvert, Gemma A and Bullmore, Edward T and Brammer, Michael J and Campbell, Ruth and Williams, Steven CR and McGuire, Philip K and Woodruff, Peter WR and Iversen, Susan D and David, Anthony S},
  journal   = {science},
  title     = {Activation of auditory cortex during silent lipreading},
  year      = {1997},
  number    = {5312},
  pages     = {593--596},
  volume    = {276},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/calvert1997auditoryActivation.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2020-11-30},
}

@Article{crosse2015congruent,
  author    = {Crosse, Michael J and Butler, John S and Lalor, Edmund C},
  journal   = {Journal of Neuroscience},
  title     = {Congruent visual speech enhances cortical entrainment to continuous auditory speech in noise-free conditions},
  year      = {2015},
  issn      = {0270-6474},
  number    = {42},
  pages     = {14195--14204},
  volume    = {35},
  doi       = {10.1523/jneurosci.1829-15.2015},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/crosse2015congruent.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Soc Neuroscience},
  timestamp = {2020-11-30},
}

@Manual{data.table,
  author = {Matt Dowle and Arun Srinivasan},
  title  = {data.table: Extension of `data.frame`},
  note   = {R package version 1.14.0},
  url    = {https://CRAN.R-project.org/package=data.table},
  year   = {2021},
}

@Article{du2016increased,
  author    = {Du, Yi and Buchsbaum, Bradley R and Grady, Cheryl L and Alain, Claude},
  title     = {Increased activity in frontal motor cortex compensates impaired speech perception in older adults},
  doi       = {10.1038/ncomms12241},
  issn      = {2041-1723},
  number    = {1},
  pages     = {1--12},
  volume    = {7},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/du2016.pdf:PDF},
  journal   = {Nature communications},
  publisher = {Nature Publishing Group},
  timestamp = {2021-05-14},
  year      = {2016},
}

@Article{dunabeitia2018multipic,
  author    = {Du{\~n}abeitia, Jon Andoni and Crepaldi, Davide and Meyer, Antje S and New, Boris and Pliatsikas, Christos and Smolka, Eva and Brysbaert, Marc},
  title     = {MultiPic: A standardized set of 750 drawings with norms for six European languages},
  doi       = {10.1080/17470218.2017.1310261},
  issn      = {1747-0218},
  number    = {4},
  pages     = {808--816},
  url       = {https://www.bcbl.eu/databases/multipic/},
  volume    = {71},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/MultiPic.pdf:PDF},
  journal   = {Quarterly Journal of Experimental Psychology},
  month     = jan,
  publisher = {Sage Publications Sage UK: London, England},
  timestamp = {2021-05-14},
  year      = {2018},
}

@Article{eg2015audiovisual,
  author    = {Eg, Ragnhild and Griwodz, Carsten and Halvorsen, P{\aa}l and Behne, Dawn},
  journal   = {Multimedia Tools and Applications},
  title     = {Audiovisual robustness: exploring perceptual tolerance to asynchrony and quality distortion},
  year      = {2015},
  issn      = {0167-6393},
  month     = feb,
  number    = {2},
  pages     = {345--365},
  volume    = {74},
  doi       = {10.1016/j.specom.2014.10.001},
  file      = {:eg2014.pdf:PDF},
  publisher = {Springer},
  timestamp = {2021-05-06},
}

@Manual{emmeans,
  author = {Russell V. Lenth},
  title  = {emmeans: Estimated Marginal Means, aka Least-Squares Means},
  note   = {R package version 1.6.0},
  url    = {https://CRAN.R-project.org/package=emmeans},
  year   = {2021},
}

@Article{esteve2018prosody,
  author    = {Esteve-Gibert, N{\'u}ria and Guella{\"\i}, Bahia},
  title     = {Prosody in the auditory and visual domains: A developmental perspective},
  doi       = {10.3389/fpsyg.2018.00338},
  issn      = {1664-1078},
  pages     = {338},
  volume    = {9},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/esteve2018prosody.pdf:PDF},
  journal   = {Frontiers in Psychology},
  publisher = {Frontiers},
  timestamp = {2021-05-14},
  year      = {2018},
}

@Article{ffmpeg,
  author    = {Tomar, Suramya},
  title     = {Converting video formats with FFmpeg},
  number    = {146},
  pages     = {10},
  volume    = {2006},
  groups    = {Materials},
  journal   = {Linux Journal},
  publisher = {Belltown Media},
  timestamp = {2021-05-14},
  year      = {2006},
}

@Book{ggplot2,
  author    = {Hadley Wickham},
  title     = {ggplot2: Elegant Graphics for Data Analysis},
  isbn      = {978-3-319-24277-4},
  publisher = {Springer-Verlag New York},
  url       = {https://ggplot2.tidyverse.org},
  year      = {2016},
}

@Article{giordano2017,
  author       = {Giordano, Bruno L and Ince, Robin A A and Gross, Joachim and Schyns, Philippe G and Panzeri, Stefano and Kayser, Christoph},
  title        = {Contributions of local speech encoding and functional connectivity to audio-visual speech perception},
  doi          = {10.7554/eLife.24763},
  editor       = {Schroeder, Charles E},
  issn         = {2050-084X},
  pages        = {e24763},
  url          = {https://doi.org/10.7554/eLife.24763},
  volume       = {6},
  abstract     = {Seeing a speaker’s face enhances speech intelligibility in adverse environments. We investigated the underlying network mechanisms by quantifying local speech representations and directed connectivity in MEG data obtained while human participants listened to speech of varying acoustic SNR and visual context. During high acoustic SNR speech encoding by temporally entrained brain activity was strong in temporal and inferior frontal cortex, while during low SNR strong entrainment emerged in premotor and superior frontal cortex. These changes in local encoding were accompanied by changes in directed connectivity along the ventral stream and the auditory-premotor axis. Importantly, the behavioral benefit arising from seeing the speaker’s face was not predicted by changes in local encoding but rather by enhanced functional connectivity between temporal and inferior frontal cortex. Our results demonstrate a role of auditory-frontal interactions in visual speech representations and suggest that functional connectivity along the ventral pathway facilitates speech comprehension in multisensory environments.},
  article_type = {journal},
  citation     = {eLife 2017;6:e24763},
  file         = {:C\:/Users/aron/Google Drive/BA/thesis/literature/giordano2017localspeechencoding.pdf:PDF},
  groups       = {Multisensory Integration, Perception Enhancement},
  journal      = {eLife},
  keywords     = {auditory system, magnetoencephalography, audio-visual speech entrainment, directed functional connectivity, inferior frontal gyrus, premotor cortex},
  month        = jun,
  pub_date     = {2017-06-07},
  publisher    = {eLife Sciences Publications, Ltd},
  timestamp    = {2021-05-12},
  year         = {2017},
}

@Article{goehring2018tolerable,
  author    = {Goehring, Tobias and Chapman, Josie L and Bleeck, Stefan and Monaghan*, Jessica JM},
  title     = {Tolerable delay for speech production and perception: effects of hearing ability and experience with hearing aids},
  number    = {1},
  pages     = {61--68},
  volume    = {57},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/goehring2017.pdf:PDF},
  journal   = {International journal of audiology},
  publisher = {Taylor \& Francis},
  timestamp = {2021-05-14},
  year      = {2018},
}

@Article{Grant2004,
  author    = {Ken W. Grant and Virginie van Wassenhove and David Poeppel},
  journal   = {Speech Communication},
  title     = {Detection of auditory (cross-spectral) and auditory–visual (cross-modal) synchrony},
  year      = {2004},
  issn      = {0167-6393},
  month     = oct,
  note      = {Special Issue on Audio Visual speech processing},
  number    = {1},
  pages     = {43--53},
  volume    = {44},
  abstract  = {Detection thresholds for temporal synchrony in auditory and auditory–visual sentence materials were obtained on normal-hearing subjects. For auditory conditions, thresholds were determined using an adaptive-tracking procedure to control the degree of temporal asynchrony of a narrow audio band of speech, both positive and negative in separate tracks, relative to three other narrow audio bands of speech. For auditory–visual conditions, thresholds were determined in a similar manner for each of four narrow audio bands of speech as well as a broadband speech condition, relative to a video image of a female speaker. Four different auditory filter conditions, as well as a broadband auditory–visual speech condition, were evaluated in order to determine whether detection thresholds were dependent on the spectral content of the acoustic speech signal. Consistent with previous studies of auditory–visual speech recognition which showed a broad, asymmetrical range of temporal synchrony for which intelligibility was basically unaffected (audio delays roughly between −40ms and +240ms), auditory–visual synchrony detection thresholds also showed a broad, asymmetrical pattern of similar magnitude (audio delays roughly between −45ms and +200ms). No differences in synchrony thresholds were observed for the different filtered bands of speech, or for broadband speech. In contrast, detection thresholds for audio-alone conditions were much smaller (between −17ms and +23ms) and symmetrical. These results suggest a fairly tight coupling between a subject’s ability to detect cross-spectral (auditory) and cross-modal (auditory–visual) asynchrony and the intelligibility of auditory and auditory–visual speech materials.},
  doi       = {https://doi.org/10.1016/j.specom.2004.06.004},
  file      = {:grant2004.pdf:PDF},
  keywords  = {Spectro-temporal asynchrony, Cross-modal asynchrony, auditory–visual speech processing},
  owner     = {Aron Petau},
  publisher = {Elsevier {BV}},
  timestamp = {2021-05-06},
  url       = {https://www.sciencedirect.com/science/article/pii/S0167639304001013},
}

@Article{haas1972the,
  author  = {haas, helmut},
  title   = {the influence of a single echo on the audibility of speech},
  number  = {2},
  pages   = {146--159},
  volume  = {20},
  journal = {journal of the audio engineering society},
  month   = mar,
  year    = {1972},
}

@Article{Hay-McCutcheon2009Asynchrony,
  author    = {Marcia J. Hay-McCutcheon and David B. Pisoni and Kristopher K. Hunt},
  title     = {Audiovisual asynchrony detection and speech perception in hearing-impaired listeners with cochlear implants: A preliminary analysis},
  doi       = {10.1080/14992020802644871},
  eprint    = {https://doi.org/10.1080/14992020802644871},
  number    = {6},
  pages     = {321--333},
  url       = {https://doi.org/10.1080/14992020802644871},
  volume    = {48},
  abstract  = {This preliminary study examined the effects of hearing loss and aging on the detection of AV asynchrony in hearing-impaired listeners with cochlear implants. Additionally, the relationship between AV asynchrony detection skills and speech perception was assessed. Individuals with normal-hearing and cochlear implant recipients were asked to make judgments about the synchrony of AV speech. The cochlear implant recipients also completed three speech perception tests, the CUNY, HINT sentences, and the CNC test. No significant differences were observed in the detection of AV asynchronous speech between the normal-hearing listeners and the cochlear implant recipients. Older adults in both groups displayed wider timing windows, over which they identified AV asynchronous speech as being synchronous, than younger adults. For the cochlear implant recipients, no relationship between the size of the temporal asynchrony window and speech perception performance was observed. The findings from this preliminary experiment suggest that aging has a greater effect on the detection of AV asynchronous speech than the use of a cochlear implant. Additionally, the temporal width of the AV asynchrony function was not correlated with speech perception skills for hearing-impaired individuals who use cochlear implants.},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Hay-McCutcheon2009Asynchrony.pdf:PDF},
  journal   = {International Journal of Audiology},
  publisher = {Taylor & Francis},
  timestamp = {2021-05-14},
  year      = {2009},
}

@Article{HillockDunn2016,
  author       = {Andrea Hillock-Dunn and D. Wesley Grantham and Mark T. Wallace},
  date         = {2016},
  journaltitle = {Neuropsychologia},
  title        = {The temporal binding window for audiovisual speech: Children are like little adults},
  doi          = {https://doi.org/10.1016/j.neuropsychologia.2016.02.017},
  issn         = {0028-3932},
  note         = {Special Issue: Synaesthesia and Multisensory Processes},
  pages        = {74--82},
  url          = {https://www.sciencedirect.com/science/article/pii/S0028393216300525},
  volume       = {88},
  abstract     = {During a typical communication exchange, both auditory and visual cues contribute to speech comprehension. The influence of vision on speech perception can be measured behaviorally using a task where incongruent auditory and visual speech stimuli are paired to induce perception of a novel token reflective of multisensory integration (i.e., the McGurk effect). This effect is temporally constrained in adults, with illusion perception decreasing as the temporal offset between the auditory and visual stimuli increases. Here, we used the McGurk effect to investigate the development of the temporal characteristics of audiovisual speech binding in 7–24 year-olds. Surprisingly, results indicated that although older participants perceived the McGurk illusion more frequently, no age-dependent change in the temporal boundaries of audiovisual speech binding was observed.},
  file         = {:hillock-dunn2016.pdf:PDF},
  keywords     = {Development, Maturation, Auditory, Visual, Multisensory, Temporal, Mcgurk effect},
  owner        = {aron},
  timestamp    = {2021-05-14},
}

@Article{Ikuta2016,
  author       = {Nobuhiko Ikuta and Ryoichiro Iwanaga and Akiko Tokunaga and Hideyuki Nakane and Koji Tanaka and Goro Tanaka},
  date         = {2016},
  journaltitle = {Hong Kong Journal of Occupational Therapy},
  title        = {Effectiveness of Earmuffs and Noise-cancelling Headphones for Coping with Hyper-reactivity to Auditory Stimuli in Children with Autism Spectrum Disorder: A Preliminary Study},
  doi          = {10.1016/j.hkjot.2016.09.001},
  eprint       = {https://doi.org/10.1016/j.hkjot.2016.09.001},
  note         = {PMID: 30186064},
  number       = {1},
  pages        = {24--32},
  url          = {https://doi.org/10.1016/j.hkjot.2016.09.001},
  volume       = {28},
  abstract     = {Objective/BackgroundThe purpose of this pilot study was to examine the effectiveness of standard earmuffs and noise-cancelling (NC) headphones in controlling behavioural problems related to hyper-reactivity to auditory stimuli in children with autism spectrum disorder (ASD).MethodsTwenty-one children with ASD aged 4–16 years (16 boys and 5 girls), after a 2-week nonwearing baseline period, were asked to use standard earmuffs and NC headphones for 2 weeks, in a random order. Parents or teachers rated participants’ behaviours that were related to their reaction to auditory stimuli.ResultsFour participants refused to wear either the earmuffs or the NC headphones. It was found that the T-score on the Goal Attainment Scaling was significantly higher during the earmuff period than that in the baseline period (Z = 2.726, p = .006). The behaviours of 5 children with ASD improved during the NC headphone period as compared with those in the baseline period; there were no differences in the T-scores on the Goal Attainment Scaling between the NC headphone period and the baseline period (Z = 1.689, p = .091) and between the earmuff and NC headphone periods (Z = −0.451, p = .678).ConclusionThis study demonstrated the effectiveness of standard earmuffs and NC headphones in helping children with ASD to cope with problem behaviours related to hyperreactivity to auditory stimuli, therefore, children with ASD could use earmuffs to help to deal with unpleasant sensory auditory stimuli.},
  file         = {:ikuta2016.pdf:PDF},
}

@Article{ipser2017sight,
  author    = {Ipser, Alberta and Agolli, Vlera and Bajraktari, Anisa and Al-Alawi, Fatimah and Djaafara, Nurfitriani and Freeman, Elliot D},
  title     = {Sight and sound persistently out of synch: stable individual differences in audiovisual synchronisation revealed by implicit measures of lip-voice integration},
  number    = {1},
  pages     = {1--12},
  volume    = {7},
  file      = {:ipser2017.pdf:PDF},
  journal   = {Scientific Reports},
  publisher = {Nature Publishing Group},
  timestamp = {2021-05-14},
  year      = {2017},
}

@Article{Irwin2017,
  author    = {Irwin, Julia and DiBlasi, Lori},
  title     = {Audiovisual speech perception: A new approach and implications for clinical populations},
  doi       = {https://doi.org/10.1111/lnc3.12237},
  eprint    = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/lnc3.12237},
  note      = {e12237 LNCO-0650.R2},
  number    = {3},
  pages     = {e12237},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12237},
  volume    = {11},
  abstract  = {Abstract This selected overview of audiovisual (AV) speech perception examines the influence of visible articulatory information on what is heard. Thought to be a cross-cultural phenomenon that emerges early in typical language development, variables that influence AV speech perception include properties of the visual and the auditory signal, attentional demands, and individual differences. A brief review of the existing neurobiological evidence on how visual information influences heard speech indicates potential loci, timing, and facilitatory effects of AV over auditory only speech. The current literature on AV speech in certain clinical populations (individuals with an autism spectrum disorder, developmental language disorder, or hearing loss) reveals differences in processing that may inform interventions. Finally, a new method of assessing AV speech that does not require obvious cross-category mismatch or auditory noise was presented as a novel approach for investigators.},
  file      = {:irwin2017.pdf:PDF},
  journal   = {Language and Linguistics Compass},
  timestamp = {2021-05-14},
  year      = {2017},
}

@Article{iversen2015synchronization,
  author    = {Iversen, John R and Patel, Aniruddh D and Nicodemus, Brenda and Emmorey, Karen},
  journal   = {Cognition},
  title     = {Synchronization to auditory and visual rhythms in hearing and deaf individuals},
  year      = {2015},
  issn      = {0010-0277},
  pages     = {232--244},
  volume    = {134},
  doi       = {10.1016/j.cognition.2014.10.018},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/iversen2015synchronization.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Elsevier},
  timestamp = {2020-11-30},
}

@Article{Jones2006,
  author    = {Jones, Jeffery and Jarick, Michelle},
  journal   = {Experimental brain research. Experimentelle Hirnforschung. Expérimentation cérébrale},
  title     = {Multisensory integration of speech signals: The relationship between space and time},
  year      = {2006},
  issn      = {0014-4819},
  month     = nov,
  number    = {3},
  pages     = {588--94},
  volume    = {174},
  doi       = {10.1007/s00221-006-0634-0},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/Jones2006.pdf:PDF},
  publisher = {Springer Science and Business Media {LLC}},
  timestamp = {2021-03-31},
}

@Book{kavanagh1972language,
  author       = {Kavanagh, James F and Mattingly, Ignatius G and others},
  date         = {1974-12},
  title        = {Language by ear and by eye: The relationships between speech and reading},
  doi          = {10.2307/412251},
  pages        = {762},
  publisher    = {Mit Press Cambridge, MA},
  volume       = {50},
  file         = {:kavanagh1972earandeye.pdf:PDF},
  issn         = {0097-8507},
  journal      = {Language},
  journaltitle = {Language},
  month        = dec,
  timestamp    = {2021-05-12},
  year         = {1972},
}

@Article{Keetels2006,
  author    = {Keetels, Mirjam and Vroomen, Jean},
  journal   = {Experimental brain research. Experimentelle Hirnforschung. Expérimentation cérébrale},
  title     = {The role of spatial disparity and hemifields in audio-visual temporal order judgments},
  year      = {2006},
  month     = jan,
  pages     = {635--40},
  volume    = {167},
  doi       = {10.1007/s00221-005-0067-1},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/keetels2005.pdf:PDF},
  owner     = {Aron Petau},
  timestamp = {2021-03-09},
}

@Article{klockgether2016,
  author    = {Klockgether,Stefan and van de Par,Steven},
  journal   = {The Journal of the Acoustical Society of America},
  title     = {Just noticeable differences of spatial cues in echoic and anechoic acoustical environments},
  year      = {2016},
  number    = {4},
  pages     = {EL352-EL357},
  volume    = {140},
  doi       = {10.1121/1.4964844},
  eprint    = {https://doi.org/10.1121/1.4964844},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/klockgether2016.pdf:PDF},
  timestamp = {2020-11-30},
  url       = {https://doi.org/10.1121/1.4964844},
}

@Article{Lauzon2020,
  author       = {Lauzon, S{\'e}bastien {\'A}. and Abraham, Arin. E. and Curcin, Kristina and Stevenson, Ryan A.},
  date         = {2020},
  journaltitle = {bioRxiv},
  title        = {The relationship between multisensory associative learning and multisensory integration},
  doi          = {10.1101/2020.08.28.272633},
  eprint       = {https://www.biorxiv.org/content/early/2020/08/29/2020.08.28.272633.full.pdf},
  url          = {https://www.biorxiv.org/content/early/2020/08/29/2020.08.28.272633},
  abstract     = {Our perception of the world around us is inherently multisensory, and integrating sensory information from multiple modalities leads to more precise and efficient perception and behaviour. Determining which sensory information from different modalities should be perceptually bound is a key component of multisensory integration. To accomplish this feat, our sensory systems rely on both low-level stimulus features, as well as multisensory associations learned throughout development based on the statistics of our environment. The present study explored the relationship between multisensory associative learning and multisensory integration using encephalography (EEG) and behavioural measures. Sixty-one participants completed a three-phase study. First, participants were exposed to novel pairings audiovisual shape-tone pairings with frequent and infrequent stimulus pairings and complete a target detection task. EEG recordings of the mismatch negativity (MMN) and P3 were calculated as neural indices of multisensory associative learning. Next, the same learned stimulus pairs presented in audiovisual as well as unisensory auditory and visual modalities while both early (\&lt;120 ms) and late neural indices of multisensory integration were recorded. Finally, participants completed an analogous behavioural speeded-response task, with behavioural indices of multisensory gain calculated using the race model. Significant relationships were found in fronto-central and occipital areas between neural measures of associative learning and both early and late indices of multisensory integration in frontal and centro-parietal areas, respectively. Participants who showed stronger indices of associative learning also exhibited stronger indices of multisensory integration of the stimuli they learned to associate. Furthermore, a significant relationship was found between neural index of early multisensory integration and behavioural indices of multisensory gain. These results provide insight into the neural underpinnings of how higher-order processes such as associative learning guide multisensory integration.Competing Interest StatementThe authors have declared no competing interest.},
  elocation-id = {2020.08.28.272633},
  file         = {:lauzon2020.pdf:PDF},
  owner        = {aron},
  publisher    = {Cold Spring Harbor Laboratory},
  timestamp    = {2021-05-14},
}

@Article{Lezzoum2014,
  author       = {Lezzoum, Narimene and Gagnon, Ghyslain and Voix, Jérémie},
  date         = {2014-11},
  journaltitle = {IEEE Transactions on Consumer Electronics},
  title        = {Voice activity detection system for smart earphones},
  doi          = {10.1109/TCE.2014.7027350},
  issn         = {1558-4127},
  number       = {4},
  pages        = {737--744},
  volume       = {60},
  abstract     = {This paper presents a real-time voice activity detection (VAD) algorithm implemented in a miniature Digital Signal Processor (DSP) for in-ear listening devices such as earphones or headphones. This system allows consumers to hear external speech signals such as public announcements or oral communication while listening to music without removing their listening devices. The proposed algorithm uses two normalized energy features that compare the energy in the frequency region containing speech information with the frequency regions typically containing noise. The extraction of the normalized features represents the key of the proposed VAD since it eliminates the need for a signal-to-noise ratio (SNR) estimator. The VAD's decision is made using two threshold comparison rules computed from the normalized features and a hangover scheme triggered after a given number of observations. The algorithm parameters, namely the frequency regions' boundaries, number of observations, two decision thresholds and hangover's duration, have been optimized offline using a genetic algorithm. The performance of the proposed VAD is compared to a benchmark algorithm in four noise environments and three SNRs. Results show that the average false positive rate (FPR) of the proposed algorithm is 4.2% and the average true positive rate (TPR) is 91.4 % compared to the benchmark algorithm which has a FPR average of 29.9 % and a TPR average of 79.0 %. The proposed VAD is implemented in hardware to validate its reliability and complexity.},
}

@Article{lezzoum2016threshold,
  author    = {Narimene Lezzoum and Ghyslain Gagnon and Jérémie Voix},
  title     = {Echo threshold between passive and electro-acoustic transmission paths in digital hearing protection devices},
  doi       = {https://doi.org/10.1016/j.ergon.2016.04.004},
  issn      = {0169-8141},
  pages     = {372--379},
  url       = {http://www.sciencedirect.com/science/article/pii/S0169814116300233},
  volume    = {53},
  abstract  = {Electronic hearing protection devices are increasingly used in noisy environments. Theses devices feature a miniaturized external microphone and internal loudspeaker in addition to an analog or digital electronic circuit. They can transmit useful audio signals such as speech and warning signals to the protected ear and can reduce the sound pressure level using dynamic range compression. In the case of a digital electronic circuit, the transmission of audio signals may be noticeably delayed because of the latency introduced by the digital signal processor and by the analog-to-digital and digital-to-analog converters. These delayed audio signals will hence interfere with the audio signals perceived naturally through the passive acoustical path of the device. The proposed study presents an original procedure to evaluate, for two representative passive earplugs, the shortest delay at which human listeners start to perceive two sounds composed of the signal transmitted through the electronic circuit and the passively transmitted signal. This shortest delay is called the echo threshold and represents the delay between the time of perception of one fused sound from two separate sounds. In this study, a transient signal, a clean speech signal, a speech signal corrupted by factory noise, and a speech signal corrupted by babble noise are used to determine the echo thresholds of the two earplugs. Twenty untrained listeners participated in this study, and were asked to determine the echo thresholds using a test software in which attenuated signals are delayed from the original signals in real-time. The findings show that when using hearing devices, the echo threshold depends on four parameters: (a) the attenuation function of the device, (b) the duration of the signal, (c) the level of the background noise and (d) the type of background noise. Defined here as the shortest time delay at which at least 20% of the participants noticed an echo, the echo threshold was found to be 8 ms for a bell signal, 16 ms for clean speech and 22 ms for speech corrupted by babble noise when using a shallow earplug fit. When using a deep fit, the echo threshold was found to be 18 ms for a bell signal and 26 ms for clean speech and 68 ms for speech in factory. No echo threshold could be clearly determined for the speech signal in babble noise with a deep earplug fit.},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Lezzoum2016EchoThreshold between Passive and Electro-Acousti.pdf:PDF},
  journal   = {International Journal of Industrial Ergonomics},
  keywords  = {Hearing protection device, Echo threshold, Just noticeable difference, Delay},
  timestamp = {2021-05-14},
  year      = {2016},
}

@Article{Li2021,
  author    = {Li, Shao and Ding, Qi and Yuan, Yichen and Yue, Zhenzhu},
  journal   = {Frontiers in Psychology},
  title     = {Audio-Visual Causality and Stimulus Reliability Affect Audio-Visual Synchrony Perception},
  year      = {2021},
  issn      = {1664-1078},
  month     = feb,
  pages     = {395},
  volume    = {12},
  abstract  = {People can discriminate the synchrony between audio-visual scenes. However, the sensitivity of audio-visual synchrony perception can be affected by many factors. Using a simultaneity judgment task, the present study investigated whether the synchrony perception of complex audio-visual stimuli was affected by audio-visual causality and stimulus reliability. In Experiment 1, the results showed that audio-visual causality could increase one's sensitivity to audio-visual onset asynchrony (AVOA) of both action stimuli and speech stimuli. Moreover, participants were more tolerant of AVOA of speech stimuli than that of action stimuli in the high causality condition, whereas no significant difference between these two kinds of stimuli was found in the low causality condition. In Experiment 2, the speech stimuli were manipulated with either high or low stimulus reliability. The results revealed a significant interaction between audio-visual causality and stimulus reliability. Under the low causality condition, the percentage of “synchronous” responses of audio-visual intact stimuli was significantly higher than that of visual_intact/auditory_blurred stimuli and audio-visual blurred stimuli. In contrast, no significant difference among all levels of stimulus reliability was observed under the high causality condition. Our study supported the synergistic effect of top-down processing and bottom-up processing in audio-visual synchrony perception.},
  doi       = {10.3389/fpsyg.2021.629996},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/li2021.pdf:PDF},
  owner     = {Aron Petau},
  publisher = {Frontiers Media {SA}},
  timestamp = {2021-03-09},
  url       = {https://www.frontiersin.org/article/10.3389/fpsyg.2021.629996},
}

@Article{lmerTest,
  author  = {Alexandra Kuznetsova and Per B. Brockhoff and Rune H. B. Christensen},
  title   = {{lmerTest} Package: Tests in Linear Mixed Effects Models},
  doi     = {10.18637/jss.v082.i13},
  number  = {13},
  pages   = {1--26},
  volume  = {82},
  journal = {Journal of Statistical Software},
  year    = {2017},
}

@Article{Macdonald1978mcgurkeffect,
  author                  = {Macdonald, J. and McGurk, H.},
  title                   = {Visual influences on speech perception processes},
  doi                     = {10.3758/BF03206096},
  issn                    = {00315117},
  language                = {English},
  note                    = {cited By 284},
  number                  = {3},
  pages                   = {253--257},
  url                     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018388657&doi=10.3758%2fBF03206096&partnerID=40&md5=c6d275c9b9a1362aeacf87e25a494621},
  volume                  = {24},
  abbrev_source_title     = {Percept. Psychophys.},
  abstract                = {An experiment is reported, the results of which confirm and extend an earlier observation that visual information for the speaker's lip movements profoundly modifies the auditorv perception of natural speech by normally hearing subjects. The effect is most pronounced when there is auditory information for a bilabial utterance combined with visual information for a nonlabial utterance. However, the effect is also obtained with the reverse combination, although to a lesser extent. These findings are considered for their relevance to auditory theories of speech perception. © 1978 Psychonomic Society, Inc.},
  affiliation             = {Department of Psychology, University of Surrey, Guildford, GU2 5XH, Surrey, United Kingdom},
  coden                   = {PEPSB},
  correspondence_address1 = {McGurk, H.; Department of Psychology, University of Surrey, Guildford, GU2 5XH, Surrey, United Kingdom},
  document_type           = {Article},
  file                    = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Macdonald-McGurk1978VisualInfluencesOnSpeechPercep.pdf:PDF},
  groups                  = {Multisensory Integration},
  journal                 = {Perception \& Psychophysics},
  keywords                = {auditory system; central nervous system; human cell; normal human; visual system, Adolescent; Adult; Female; Human; Lipreading; Male; Phonetics; Speech Perception; Visual Perception},
  pubmed_id               = {704285},
  source                  = {Scopus},
  timestamp               = {2021-05-14},
  year                    = {1978},
}

@Article{Maier2011asynchrony,
  author    = {Maier, Joost and Di Luca, Massimiliano and Noppeney, Uta},
  title     = {Audiovisual Asynchrony Detection in Human Speech},
  doi       = {10.1037/a0019952},
  pages     = {245--56},
  volume    = {37},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Maier2011-audiovisual-asynchrony-speech.pdf:PDF},
  journal   = {Journal of experimental psychology. Human perception and performance},
  month     = feb,
  timestamp = {2021-05-14},
  year      = {2011},
}

@Book{MASS,
  author    = {W. N. Venables and B. D. Ripley},
  title     = {Modern Applied Statistics with S},
  edition   = {Fourth},
  note      = {ISBN 0-387-95457-0},
  publisher = {Springer},
  url       = {https://www.stats.ox.ac.uk/pub/MASS4/},
  address   = {New York},
  year      = {2002},
}

@Article{massaro1996conflicting,
  author    = {Massaro,Dominic W. and Cohen,Michael M. and Smeele,Paula M. T.},
  journal   = {The Journal of the Acoustical Society of America},
  title     = {Perception of asynchronous and conflicting visual and auditory speech},
  year      = {1996},
  month     = sep,
  number    = {3},
  pages     = {1777--1786},
  volume    = {100},
  doi       = {10.1121/1.417342},
  eprint    = {https://doi.org/10.1121/1.417342},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/Massaro et al. - 1996 - Perception of asynchronous and conflicting visual .pdf:PDF},
  owner     = {Aron Petau},
  publisher = {Acoustical Society of America ({ASA})},
  timestamp = {2021-03-07},
  url       = {https://doi.org/10.1121/1.417342},
}

@Book{matlab,
  author    = {MATLAB},
  title     = {9.9.0.1592791 (R2020b) Update 5)},
  publisher = {The MathWorks Inc.},
  address   = {Natick, Massachusetts},
  groups    = {Materials},
  timestamp = {2021-05-14},
  year      = {2020},
}

@Article{mcgurk1976hearing,
  author    = {McGurk, Harry and MacDonald, John},
  journal   = {Nature},
  title     = {Hearing lips and seeing voices},
  year      = {1976},
  issn      = {0028-0836},
  month     = dec,
  number    = {5588},
  pages     = {746--748},
  volume    = {264},
  doi       = {10.1038/264746a0},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/mcgurk1976hearing.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Nature Publishing Group},
  timestamp = {2021-04-03},
}

@Book{McNeill1992,
  author    = {McNeill, David},
  title     = {Hand and mind: What gestures reveal about thought},
  publisher = {University of Chicago press},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/mcneill1992hand.pdf:PDF},
  timestamp = {2021-03-07},
  year      = {1992},
}

@Article{meredith1986visual,
  author    = {Meredith, M Alex and Stein, Barry E},
  title     = {Visual, auditory, and somatosensory convergence on cells in superior colliculus results in multisensory integration},
  doi       = {10.1152/jn.1986.56.3.640},
  issn      = {0022-3077},
  number    = {3},
  pages     = {640--662},
  volume    = {56},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/meredith1986.pdf:PDF},
  groups    = {Multisensory Integration},
  journal   = {Journal of neurophysiology},
  publisher = {American Physiological Society Bethesda, MD},
  timestamp = {2021-05-14},
  year      = {1986},
}
Key : classical study on multisensory integration

@Article{nakamura2002integration,
  author    = {S. Nakamura},
  journal   = {IEEE Transactions on Neural Networks},
  title     = {Statistical multimodal integration for audio-visual speech processing},
  year      = {2002},
  issn      = {1045-9227},
  number    = {4},
  pages     = {854--866},
  volume    = {13},
  doi       = {10.1109/tnn.2002.1021886},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/nakamura2002.pdf:PDF},
  groups    = {Multisensory Integration},
  timestamp = {2020-11-30},
}

@Article{noel2017atypical,
  author    = {Noel, Jean-Paul and De Niear, Matthew A and Stevenson, Ryan and Alais, David and Wallace, Mark T},
  title     = {Atypical rapid audio-visual temporal recalibration in autism spectrum disorders},
  number    = {1},
  pages     = {121--129},
  volume    = {10},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Noel2017 Atypical rapid audio-visual temporal recalibration.pdf:PDF},
  groups    = {Multisensory Integration},
  journal   = {Autism Research},
  publisher = {Wiley Online Library},
  timestamp = {2021-05-14},
  year      = {2017},
}

Key : handedness test

@Article{oldfield1971handedness,
  author    = {R.C. Oldfield},
  title     = {The assessment and analysis of handedness: The Edinburgh inventory},
  doi       = {https://doi.org/10.1016/0028-3932(71)90067-4},
  issn      = {0028-3932},
  number    = {1},
  pages     = {97--113},
  url       = {http://www.sciencedirect.com/science/article/pii/0028393271900674},
  volume    = {9},
  abstract  = {The need for a simply applied quantitative assessment of handedness is discussed and some previous forms reviewed. An inventory of 20 items with a set of instructions and response- and computational-conventions is proposed and the results obtained from a young adult population numbering some 1100 individuals are reported. The separate items are examined from the point of view of sex, cultural and socio-economic factors which might appertain to them and also of their inter-relationship to each other and to the measure computed from them all. Criteria derived from these considerations are then applied to eliminate 10 of the original 20 items and the results recomputed to provide frequency-distribution and cumulative frequency functions and a revised item-analysis. The difference of incidence of handedness between the sexes is discussed.},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/oldfield1971handedness.pdf:PDF;:C\:/Users/aron/Google Drive/BA/thesis/literature/beker2018.pdf:PDF;:C\:/Users/aron/Google Drive/BA/thesis/literature/beker2018.pdf:PDF},
  journal   = {Neuropsychologia},
  timestamp = {2021-05-14},
  year      = {1971},
}

@InCollection{paraskevoudi2019perception,
  author    = {Paraskevoudi, Nadia and Vatakis, Argiro},
  booktitle = {The Illusions of Time},
  date      = {2019},
  title     = {When the Perception of a Synchronous World Is—Mostly—Just an Illusion},
  doi       = {10.1007/978-3-030-22048-8_13},
  pages     = {225--257},
  publisher = {Springer},
  timestamp = {2021-05-14},
  year      = {2019},
}

@Article{peirce2019psychopy2,
  author    = {Peirce, Jonathan and Gray, Jeremy R and Simpson, Sol and MacAskill, Michael and H{\"o}chenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindel{\o}v, Jonas Kristoffer},
  title     = {PsychoPy2: Experiments in behavior made easy},
  doi       = {10.3758/s13428-018-01193-y},
  issn      = {1554-3528},
  number    = {1},
  pages     = {195--203},
  volume    = {51},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/Peirce2019_Article_PsychoPy2ExperimentsInBehavior.pdf:PDF},
  journal   = {Behavior research methods},
  month     = feb,
  publisher = {Springer},
  timestamp = {2021-05-14},
  year      = {2019},
}

@Article{Petrini2009,
  author    = {Petrini, Karin and Dahl, Sofia and Rocchesso, Davide and Waadeland, Carl and Avanzini, Federico and Puce, Aina and Pollick, Frank},
  journal   = {Experimental brain research. Experimentelle Hirnforschung. Expérimentation cérébrale},
  title     = {Multisensory integration of drumming actions: Musical expertise affects perceived audiovisual asynchrony},
  year      = {2009},
  month     = may,
  pages     = {339--52},
  volume    = {198},
  doi       = {10.1007/s00221-009-1817-2},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/petrini2009.pdf:PDF},
  owner     = {Aron Petau},
  timestamp = {2021-03-09},
}

@Article{Pfeiffer2019,
  author       = {Pfeiffer, Beth and Stein Duker, Leah and Murphy, AnnMarie and Shui, Chengshi},
  date         = {2019-11},
  journaltitle = {Frontiers in Integrative Neuroscience},
  title        = {Effectiveness of Noise-Attenuating Headphones on Physiological Responses for Children With Autism Spectrum Disorders},
  doi          = {10.3389/fnint.2019.00065},
  pages        = {65},
  volume       = {13},
  file         = {:pfeiffer2018.pdf:PDF},
}

@Article{pollack1954visual,
  author    = {W. H. Sumby and Irwin Pollack},
  journal   = {The Journal of the Acoustical Society of America},
  title     = {Visual Contribution to Speech Intelligibility in Noise},
  year      = {1954},
  issn      = {0001-4966},
  month     = mar,
  number    = {2},
  pages     = {212--215},
  volume    = {26},
  doi       = {10.1121/1.1907309},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/sumby1954.pdf:PDF},
  publisher = {Acoustical Society of America ({ASA})},
  timestamp = {2021-03-07},
}

@Article{pouw2019entrainment,
  author    = {Pouw, Wim and Dixon, James A},
  title     = {Entrainment and modulation of gesture--speech synchrony under delayed auditory feedback},
  doi       = {10.1111/cogs.12721},
  issn      = {0364-0213},
  number    = {3},
  pages     = {e12721},
  volume    = {43},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/pouw2019entrainment.pdf:PDF},
  journal   = {Cognitive Science},
  publisher = {Wiley Online Library},
  timestamp = {2021-05-14},
  year      = {2019},
}

@Misc{puredata,
  author    = {puredata},
  title     = {puredata.info},
  groups    = {Materials},
  timestamp = {2021-05-14},
  year      = {2020},
}

@Article{quene2007justnoticeabledifference,
  author    = {Hugo Quené},
  title     = {On the just noticeable difference for tempo in speech},
  doi       = {https://doi.org/10.1016/j.wocn.2006.09.001},
  issn      = {0095-4470},
  number    = {3},
  pages     = {353--362},
  url       = {http://www.sciencedirect.com/science/article/pii/S0095447006000441},
  volume    = {35},
  abstract  = {Speakers vary their speech tempo (speaking rate), and such variations in tempo are quite noticeable. But what is the just noticeable difference (JND) for tempo in speech? The present study aims at providing a realistic and robust estimate, by using multiple speech tokens from multiple speakers. The JND is assessed in two (2IAX and 2IFC) comparison experiments, yielding an estimated JND for speech tempo of about 5%. A control experiment suggests that this finding is not due to acoustic artefacts of the tempo-transformation method used. Tempo variations within speakers typically exceed this JND, which makes such variations relevant in speech communication.},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/quene2007justnoticeabledifference.pdf:PDF},
  groups    = {Multisensory Integration},
  journal   = {Journal of Phonetics},
  timestamp = {2021-05-14},
  year      = {2007},
}

@InProceedings{ren2016asynchrony,
  author    = {Y. {Ren} and W. {Yang} and Q. {Wu} and F. {Wu} and S. {Takahashi} and E. {Yoshimichi} and J. {Wu}},
  booktitle = {2016 IEEE International Conference on Mechatronics and Automation},
  title     = {Study of audiovisual asynchrony signal processing: Robot recognition system of different ages},
  doi       = {10.1109/ICMA.2016.7558927},
  pages     = {2320--2325},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/ren2016asynchrony.pdf:PDF},
  timestamp = {2021-05-14},
  year      = {2016},
}

@Manual{rmisc,
  author = {Ryan M. Hope},
  title  = {Rmisc: Rmisc: Ryan Miscellaneous},
  note   = {R package version 1.5},
  url    = {https://CRAN.R-project.org/package=Rmisc},
  year   = {2013},
}

@Manual{rmisc,
  author = {Ryan M. Hope},
  title  = {Rmisc: Rmisc: Ryan Miscellaneous},
  note   = {R package version 1.5},
  url    = {https://CRAN.R-project.org/package=Rmisc},
  year   = {2013},
}
Key:

@Article{rosemann2018audio,
  author    = {Rosemann, Stephanie and Thiel, Christiane M},
  title     = {Audio-visual speech processing in age-related hearing loss: Stronger integration and increased frontal lobe recruitment},
  doi       = {10.1016/j.neuroimage.2018.04.023},
  issn      = {1053-8119},
  pages     = {425--437},
  volume    = {175},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/RosemannThiel2018NeuroImage.pdf:PDF},
  groups    = {Multisensory Integration},
  journal   = {NeuroImage},
  publisher = {Elsevier},
  timestamp = {2021-05-14},
  year      = {2018},
}

@Article{Rosenblum2017,
  author       = {Lawrence D. Rosenblum and James W. Dias and Josh Dorsi},
  date         = {2017},
  journaltitle = {Journal of Cognitive Psychology},
  title        = {The supramodal brain: implications for auditory perception},
  doi          = {10.1080/20445911.2016.1181691},
  eprint       = {https://doi.org/10.1080/20445911.2016.1181691},
  number       = {1},
  pages        = {65--87},
  url          = {https://doi.org/10.1080/20445911.2016.1181691},
  volume       = {29},
  abstract     = {ABSTRACTThe perceptual brain is designed around multisensory input. Areas once thought dedicated to a single sense are now known to work with multiple senses. It has been argued that the multisensory nature of the brain reflects a cortical architecture for which task, rather than sensory system, is the primary design principle. This supramodal thesis is supported by recent research on human echolocation and multisensory speech perception. In this review, we discuss the behavioural implications of a supramodal architecture, especially as they pertain to auditory perception. We suggest that the architecture implies a degree of perceptual parity between the senses and that cross-sensory integration occurs early and completely. We also argue that a supramodal architecture implies that perceptual experience can be shared across modalities and that this sharing should occur even without bimodal experience. We finish by briefly suggesting areas of future research.},
  file         = {:rosenblum2016.pdf:PDF},
  owner        = {aron},
  publisher    = {Routledge},
  timestamp    = {2021-05-14},
}

@Article{rosenblum2019audiovisual,
  author    = {Rosenblum, Lawrence D},
  journal   = {Oxford Research Encyclopedia of Linguistics},
  title     = {Audiovisual speech perception and the McGurk effect},
  year      = {2019},
  doi       = {10.1093/acrefore/9780199384655.013.420},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/rosenblum2019audiovisual.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Oxford Research Encyclopedia of Linguistics},
  timestamp = {2021-01-22},
}

@Article{ross2007you,
  author    = {Ross, Lars A and Saint-Amour, Dave and Leavitt, Victoria M and Javitt, Daniel C and Foxe, John J},
  journal   = {Cerebral cortex},
  title     = {Do you see what I am saying? Exploring visual enhancement of speech comprehension in noisy environments},
  year      = {2007},
  issn      = {0920-9964},
  number    = {5},
  pages     = {1147--1153},
  volume    = {17},
  doi       = {10.1016/j.schres.2007.08.008},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/ross2006-noisy.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Oxford University Press},
  timestamp = {2020-11-30},
}

@Manual{rstudio,
  author       = {{R Core Team}},
  date         = {2021},
  title        = {R: A Language and Environment for Statistical Computing},
  organization = {R Foundation for Statistical Computing},
  url          = {https://www.R-project.org/},
  address      = {Vienna, Austria},
  groups       = {Materials},
  timestamp    = {2021-05-14},
  year         = {2021},
}

@Article{sakamoto2018,
  author    = {Shuichi Sakamoto and Zhenglie Cui and Tomori Miyashita and Masayuki Morimoto and Yôiti Suzuki and Hayato Sato},
  title     = {Effects of inter-word pauses on speech intelligibility under long-path echo conditions},
  doi       = {https://doi.org/10.1016/j.apacoust.2018.01.020},
  issn      = {0003-682X},
  pages     = {263--274},
  url       = {http://www.sciencedirect.com/science/article/pii/S0003682X17310575},
  volume    = {140},
  abstract  = {Long-path echo is a salient factor that causes the degradation of the intelligibility of speech transmitted through a wide area outdoor environment or a very large indoor space using public-address systems. To robustly transmit speech information under such conditions, it is important to overcome this effect by controlling the characteristics of speech sounds. In this study, we consider the effects of inserting pauses between the words of a sentence. We performed word intelligibility tests using a series of four continuous words, called a quadruplet. Various pause lengths and long-path echo patterns were applied to the quadruplet. The results of the experiments demonstrate that word intelligibility under a long-path echo is significantly improved by the insertion of pauses between the words. Intelligibility can approach the same levels observed in the absence of echoes for a pause length of approximately 200 ms, which is almost the same as the length of 1-mora for the words used in the experiments. Moreover, this 200 ms pause is known to be sufficient to improve speech recognition in older adults. These results suggest that inter-word pauses of a length of approximately 1-mora can generally enhance the robustness of speech communication systems when used under a severe environment.},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/sakamoto2018.pdf:PDF},
  groups    = {Perception Enhancement},
  journal   = {Applied Acoustics},
  keywords  = {Word intelligibility, Long-path echo, Pause, Open-air public-address systems},
  timestamp = {2021-05-14},
  year      = {2018},
}

@Article{Samelli2018,
  author    = {Samelli, Alessandra G and Gomes, Raquel F and Chammas, Tiago V and Silva, Bárbara G and Moreira, Renata R and Fiorini, Ana C},
  title     = {The study of attenuation levels and the comfort of earplugs},
  doi       = {10.4103/nah.nah_50_17},
  issn      = {1463-1741},
  number    = {94},
  pages     = {112--119},
  url       = {https://europepmc.org/articles/PMC5965002},
  volume    = {20},
  abstract  = {&lt;h4&gt;Introduction&lt;/h4&gt;This study aimed to analyze and compare four different types of earplugs, divided into premolded plugs and foam plug models, in relation to the level of attenuation, comfort, and the size of the external acoustic meatus (EAM) in an attempt to identify how these variables influence the choice of specific hearing protection devices (HPDs).&lt;h4&gt;Materials and methods&lt;/h4&gt;A cross-sectional observational study was performed in a sample of 49 participants, oriented toward the ideal placement of four HPDs, two premolded and two foam plugs (3M™). The procedures included otoscopy, EAM diameter measurement using an otometer, EAM volume measurement via an acoustic impedance test, and the obtainment of the bilateral personal attenuation rating (PAR) for each HPD using the E-A-Rfit™ Validation System (3M™). The Bipolar Comfort Rating Scale (BCRS) instrument was applied twice for each individual: once after the evaluations with the premolded HPDs and again after the evaluations with the foam plug HPDs. Then, each participant was asked which was his/her favorite protector.&lt;h4&gt;Results&lt;/h4&gt;The volume of the EAM was not directly related to the diameter of the EAM. The attenuation did not interfere with the HPD preference, and the PAR of the foam plug was significantly higher regardless of the preferred HPD. Regarding the BCRS, the variables "Placement," "Complexity," and "Occlusion Effect" had higher scores for premolded HPDs and had a direct relationship with the type of preferred HPD.&lt;h4&gt;Conclusion&lt;/h4&gt;Attention to the use of HPDs should be personalized, taking into account the needs of each individual, considering not only the attenuation, but also the user's reported well-being.},
  journal   = {Noise \& Health},
  owner     = {aron},
  timestamp = {2021-05-12},
  year      = {2018},
}

@Manual{scales,
  author = {Hadley Wickham and Dana Seidel},
  title  = {scales: Scale Functions for Visualization},
  note   = {R package version 1.1.1},
  url    = {https://CRAN.R-project.org/package=scales},
  year   = {2020},
}

@Manual{sjPlot,
  author = {Daniel Lüdecke},
  title  = {sjPlot: Data Visualization for Statistics in Social Science},
  note   = {R package version 2.8.8},
  url    = {https://CRAN.R-project.org/package=sjPlot},
  year   = {2021},
}

@Article{soto2004assessing,
  author    = {Soto-Faraco, Salvador and Navarra, Jordi and Alsius, Agnes},
  journal   = {Cognition},
  title     = {Assessing automaticity in audiovisual speech integration: evidence from the speeded classification task},
  year      = {2004},
  issn      = {0010-0277},
  number    = {3},
  pages     = {B13--B23},
  volume    = {92},
  comment   = {Nice easy and short, provides evidence for automaticity of multimodal integration via an indirect mcGurk Effect to induce perceived inconsistency, resulting in slower reaction times},
  doi       = {10.1016/j.cognition.2003.10.005},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/soto-faraco2004-automaticity-audiovisual-integration.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Elsevier},
  timestamp = {2020-11-30},
}

@InCollection{stein1993integration,
  author    = {Barry E. Stein and M. Alex Meredith and Mark T. Wallace},
  booktitle = {Progress in Brain Research},
  title     = {Chapter 8 The visually responsive neuron and beyond: multisensory integration in cat and monkey},
  doi       = {10.1016/s0079-6123(08)60359-3},
  editor    = {T.P. Hicks and S. Molotchnikoff and T. Ono},
  pages     = {79--90},
  publisher = {Elsevier},
  series    = {Progress in Brain Research},
  url       = {http://www.sciencedirect.com/science/article/pii/S0079612308603593},
  volume    = {95},
  abstract  = {Publisher Summary
The cat superior colliculus neuron is a major site for the convergence and integration of multisensory information. It is only one of many central nervous system sites in many species where information from several modalities converges. Single cells in cat LS (lateral suprasylvian) and AES (anterior ectosylvian sulcus) and in monkey IPS (intraparietal sulcus) and STS (superior temporal sulcus) were examined. Although the spatial, temporal, and multiplicative characteristics of multisensory integration were most closely examined in cat cortex, all of the observations in monkey were consistent with those described in cat. The multisensory receptive fields of a single neuron overlapped one another in space, such that sensory stimuli that were in close spatial register fell within their excitatory receptive fields and enhanced the neuron's activity; spatially disparate stimuli produced either no interaction or depressed responses. The rules of multisensory integration evident at the level of the single neuron are also consistent with studies of intact behaving animals. The attentive and orientation responses cats make to visual and auditory stimuli were predictable based on the reactions of superior colliculus neurons to these stimuli. The data indicate that the visual responses of many neurons, whether in the superior colliculus or cortex, represent only one facet of their sensory coding capabilities.},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/stein1993.pdf:PDF},
  groups    = {Multisensory Integration},
  issn      = {0079-6123},
  timestamp = {2021-05-12},
  year      = {1993},
}

@Book{stein1993merging,
  author    = {Stein, Barry E and Meredith, M Alex},
  title     = {The merging of the senses.},
  publisher = {The MIT Press},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/stein1993.pdf:PDF},
  groups    = {Multisensory Integration},
  timestamp = {2021-05-14},
  year      = {1993},
}

@Article{stevenson2014impact,
  author    = {Stevenson, Ryan A and Segers, Magali and Ferber, Susanne and Barense, Morgan D and Wallace, Mark T},
  journal   = {Frontiers in Psychology},
  title     = {The impact of multisensory integration deficits on speech perception in children with autism spectrum disorders},
  year      = {2014},
  issn      = {1664-1078},
  month     = may,
  pages     = {379},
  volume    = {5},
  doi       = {10.3389/fpsyg.2014.00379},
  file      = {:C\:/Users/Aron/Google Drive/BA/thesis/literature/stevenson2014autismDeficits.pdf:PDF},
  publisher = {Frontiers},
  timestamp = {2021-03-07},
}

@Article{stevenson2018cascading,
  author    = {Stevenson, Ryan A and Segers, Magali and Ncube, Busisiwe L and Black, Karen R and Bebko, James M and Ferber, Susanne and Barense, Morgan D},
  title     = {The cascading influence of multisensory processing on speech perception in autism},
  number    = {5},
  pages     = {609--624},
  volume    = {22},
  file      = {:stevenson2017.pdf:PDF},
  journal   = {Autism},
  publisher = {Sage Publications Sage UK: London, England},
  timestamp = {2021-05-14},
  year      = {2018},
}

@Article{Stiegler2010,
  author       = {Lillian N. Stiegler and Rebecca Davis},
  date         = {2010},
  journaltitle = {Focus on Autism and Other Developmental Disabilities},
  title        = {Understanding Sound Sensitivity in Individuals with Autism Spectrum Disorders},
  doi          = {10.1177/1088357610364530},
  eprint       = {https://doi.org/10.1177/1088357610364530},
  number       = {2},
  pages        = {67--75},
  url          = {https://doi.org/10.1177/1088357610364530},
  volume       = {25},
  abstract     = {Literature on sound sensitivity in individuals with and without autism spectrum disorders (ASD) is reviewed in this article. Empirical evidence is examined, and physiologic and psychoemotional-behavioral perspectives are described.There is virtually no evidence of true physiological differences in auditory systems of individuals with ASD. It is evident, however, that many people with ASD (a) feel fearful and anxious about sound, and (b) may experience unpleasant physiological sensations because of autonomic and/or behavioral responses to nonpreferred sounds, but (c) can learn to react in less stigmatizing, more effectively self-regulating ways. Current assessment and intervention practices are discussed, and a case is presented. Heightened understanding of this issue among caregivers and interventionists may ultimately improve life participation for individuals with ASD.},
  file         = {:stiegler2010.pdf:PDF},
}

@Article{stilp2020,
  author    = {Stilp, Christian},
  title     = {Acoustic context effects in speech perception},
  doi       = {https://doi.org/10.1002/wcs.1517},
  eprint    = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.1517},
  number    = {1},
  pages     = {e1517},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1517},
  volume    = {11},
  abstract  = {Abstract The extreme acoustic variability of speech is well established, which makes the proficiency of human speech perception all the more impressive. Speech perception, like perception in any modality, is relative to context, and this provides a means to normalize the acoustic variability in the speech signal. Acoustic context effects in speech perception have been widely documented, but a clear understanding of how these effects relate to each other across stimuli, timescales, and acoustic domains is lacking. Here we review the influences that spectral context, temporal context, and spectrotemporal context have on speech perception. Studies are organized in terms of whether the context precedes the target (forward effects) or follows it (backward effects), and whether the context is adjacent to the target (proximal) or temporally removed from it (distal). Special cases where proximal and distal contexts have competing influences on perception are also considered. Across studies, a common theme emerges: acoustic differences between contexts and targets are perceptually magnified, producing contrast effects that facilitate perception of target sounds and words. This indicates enhanced sensitivity to changes in the acoustic environment, which maximizes the amount of potential information that can be transmitted to the perceiver. This article is categorized under: Linguistics > Language in Mind and Brain Psychology > Perception and Psychophysics},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/stilp2019.pdf:PDF},
  journal   = {WIREs Cognitive Science},
  keywords  = {context effects, speech categorization, speech perception},
  timestamp = {2021-05-14},
  year      = {2020},
}

@Article{stone2002tolerable,
  author    = {Stone, Michael A and Moore, Brian CJ},
  title     = {Tolerable hearing aid delays. II. Estimation of limits imposed during speech production},
  doi       = {10.1097/00003446-200208000-00008},
  issn      = {0196-0202},
  number    = {4},
  pages     = {325--338},
  volume    = {23},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Stone2002TolerableDelays.pdf:PDF},
  journal   = {Ear and Hearing},
  publisher = {LWW},
  timestamp = {2021-05-14},
  year      = {2002},
}

@Article{stone2008tolerable,
  author    = {Stone, Michael A and Moore, Brian CJ and Meisenbacher, Katrin and Derleth, Ralph P},
  journal   = {Ear and Hearing},
  title     = {Tolerable hearing aid delays. V. Estimation of limits for open canal fittings},
  year      = {2008},
  issn      = {0196-0202},
  month     = aug,
  number    = {4},
  pages     = {601--617},
  volume    = {29},
  doi       = {10.1097/aud.0b013e3181734ef2},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/stone2008.pdf:PDF},
  publisher = {LWW},
  timestamp = {2021-03-07},
}

@Article{stratton1896some,
  author    = {Stratton, George M},
  journal   = {Psychological review},
  title     = {Some preliminary experiments on vision without inversion of the retinal image.},
  year      = {1896},
  issn      = {1939-1471},
  month     = nov,
  number    = {6},
  pages     = {611--617},
  volume    = {3},
  doi       = {10.1037/h0072918},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/stratton1896.pdf:PDF},
  publisher = {American Psychological Association ({APA})},
  timestamp = {2021-03-07},
}

@Article{Summerfield1992,
  author       = {Summerfield, Quentin and Bruce, Vicki and Cowey, Alan and Ellis, Andrew W. and Perrett, D. I.},
  date         = {1992},
  journaltitle = {Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences},
  title        = {Lipreading and audio-visual speech perception},
  doi          = {10.1098/rstb.1992.0009},
  eprint       = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.1992.0009},
  number       = {1273},
  pages        = {71--78},
  url          = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.1992.0009},
  volume       = {335},
  abstract     = {This paper reviews progress in understanding the psychology of lipreading and audio-visual speech perception. It considers four questions. What distinguishes better from poorer lipreaders? What are the effects of introducing a delay between the acoustical and optical speech signals? What have attempts to produce computer animations of talking faces contributed to our understanding of the visual cues that distinguish consonants and vowels? Finally, how should the process of audio-visual integration in speech perception be described; that is, how are the sights and sounds of talking faces represented at their conflux?},
  file         = {:summerfield1992.pdf:PDF},
}

@Article{tidyverse,
  author  = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
  title   = {Welcome to the {tidyverse}},
  doi     = {10.21105/joss.01686},
  number  = {43},
  pages   = {1686},
  volume  = {4},
  journal = {Journal of Open Source Software},
  year    = {2019},
}

@Article{turi2016noRecalibrationAutism,
  author    = {Turi, Marco and Karaminis, Themelis and Pellicano, Elizabeth and Burr, David},
  title     = {No rapid audiovisual recalibration in adults on the autism spectrum},
  doi       = {10.1038/srep21756},
  issn      = {2045-2322},
  pages     = {21756},
  volume    = {6},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/turi2016noRecalibrationAutism.pdf:PDF},
  groups    = {Multisensory Integration},
  journal   = {Scientific reports},
  publisher = {Nature Publishing Group},
  timestamp = {2021-05-14},
  year      = {2016},
}

@Article{uslar2013stimuli,
  author    = {Uslar,Verena N. and Carroll,Rebecca and Hanke,Mirko and Hamann,Cornelia and Ruigendijk,Esther and Brand,Thomas and Kollmeier,Birger},
  title     = {Development and evaluation of a linguistically and audiologically controlled sentence intelligibility test},
  doi       = {10.1121/1.4818760},
  eprint    = {https://doi.org/10.1121/1.4818760},
  number    = {4},
  pages     = {3039--3056},
  url       = {https://doi.org/10.1121/1.4818760},
  volume    = {134},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/uslar2013OLAKS.pdf:PDF},
  journal   = {The Journal of the Acoustical Society of America},
  month     = oct,
  publisher = {Acoustical Society of America ({ASA})},
  timestamp = {2021-05-14},
  year      = {2013},
}

@Article{VanderBurg2018,
  author    = {Van der Burg, Erik and Alais, David and Cass, John},
  journal   = {Attention, Perception, \& Psychophysics},
  title     = {Rapid recalibration to audiovisual asynchrony follows the physical—not the perceived—temporal order},
  year      = {2018},
  issn      = {1943-3921},
  month     = jul,
  number    = {8},
  pages     = {2060--2068},
  volume    = {80},
  doi       = {10.3758/s13414-018-1540-9},
  file      = {:VanDerBurg2018RapidRecalibration.pdf:PDF},
  publisher = {Springer},
  timestamp = {2021-05-06},
}

@Article{vanwassenhove2007window,
  author    = {Virginie {van Wassenhove} and Ken W. Grant and David Poeppel},
  title     = {Temporal window of integration in auditory-visual speech perception},
  doi       = {https://doi.org/10.1016/j.neuropsychologia.2006.01.001},
  issn      = {0028-3932},
  note      = {Advances in Multisensory Processes},
  number    = {3},
  pages     = {598--607},
  url       = {http://www.sciencedirect.com/science/article/pii/S002839320600011X},
  volume    = {45},
  abstract  = {Forty-three normal hearing participants were tested in two experiments, which focused on temporal coincidence in auditory visual (AV) speech perception. In these experiments, audio recordings of/pa/and/ba/were dubbed onto video recordings of /ba/or/ga/, respectively (ApVk, AbVg), to produce the illusory “fusion” percepts /ta/, or /da/ [McGurk, H., & McDonald, J. (1976). Hearing lips and seeing voices. Nature, 264, 746–747]. In Experiment 1, an identification task using McGurk pairs with asynchronies ranging from −467ms (auditory lead) to +467ms was conducted. Fusion responses were prevalent over temporal asynchronies from −30ms to +170ms and more robust for audio lags. In Experiment 2, simultaneity judgments for incongruent and congruent audiovisual tokens (AdVd, AtVt) were collected. McGurk pairs were more readily judged as asynchronous than congruent pairs. Characteristics of the temporal window over which simultaneity and fusion responses were maximal were quite similar, suggesting the existence of a 200ms duration asymmetric bimodal temporal integration window.},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/vanwassenhove2007temporalWindow.pdf:PDF},
  groups    = {Multisensory Integration},
  journal   = {Neuropsychologia},
  keywords  = {McGurk illusion, Multisensory, Time, Psychophysics, Analysis-by-synthesis},
  timestamp = {2021-05-14},
  year      = {2007},
}

@Article{Vatakis2006,
  author    = {Argiro Vatakis and Charles Spence},
  journal   = {Neuroscience Letters},
  title     = {Audiovisual synchrony perception for speech and music assessed using a temporal order judgment task},
  year      = {2006},
  issn      = {0304-3940},
  month     = jan,
  number    = {1},
  pages     = {40--44},
  volume    = {393},
  abstract  = {This study investigated people's sensitivity to audiovisual asynchrony in briefly-presented speech and musical videos. A series of speech (letters and syllables) and guitar and piano music (single and double notes) video clips were presented randomly at a range of stimulus onset asynchronies (SOAs) using the method of constant stimuli. Participants made unspeeded temporal order judgments (TOJs) regarding which stream (auditory or visual) appeared to have been presented first. The accuracy of participants’ TOJ performance (measured in terms of the just noticeable difference; JND) was significantly better for the speech than for either the guitar or piano music video clips, suggesting that people are more sensitive to asynchrony for speech than for music stimuli. The visual stream had to lead the auditory stream for the point of subjective simultaneity (PSS) to be achieved in the piano music clips while auditory leads were typically required for the guitar music clips. The PSS values obtained for the speech stimuli varied substantially as a function of the particular speech sound presented. These results provide the first empirical evidence regarding people's sensitivity to audiovisual asynchrony for musical stimuli. Our results also demonstrate that people's sensitivity to asynchrony in speech stimuli is better than has been suggested on the basis of previous research using continuous speech streams as stimuli.},
  doi       = {https://doi.org/10.1016/j.neulet.2005.09.032},
  file      = {:vatakis2006.pdf:PDF},
  keywords  = {Synchrony perception, TOJ, Speech, Music, Audition, Vision},
  owner     = {Aron Petau},
  publisher = {Elsevier {BV}},
  timestamp = {2021-05-06},
  url       = {https://www.sciencedirect.com/science/article/pii/S030439400501092X},
}

@Article{Vatakis2006a,
  author       = {Argiro Vatakis and Charles Spence},
  date         = {2006},
  journaltitle = {Neuroscience Letters},
  title        = {Evaluating the influence of frame rate on the temporal aspects of audiovisual speech perception},
  doi          = {https://doi.org/10.1016/j.neulet.2006.06.041},
  issn         = {0304-3940},
  number       = {1},
  pages        = {132--136},
  url          = {https://www.sciencedirect.com/science/article/pii/S030439400600632X},
  volume       = {405},
  abstract     = {We investigated whether changing the frame rate at which speech video clips were presented (6–30 frames per second, fps) would affect audiovisual temporal perception. Participants made unspeeded temporal order judgments (TOJs) regarding which signal (auditory or visual) was presented first for video clips presented at a range of different stimulus onset asynchronies (SOAs) using the method of constant stimuli. Temporal discrimination accuracy was unaffected by changes in frame rate, while lower frame rate speech video clips required larger visual-speech leads for the point of subjective simultaneity (PSS) to be achieved than did higher frame rate video clips. The significant effect of frame rate on temporal perception demonstrated here has not been controlled for in previous studies of audiovisual synchrony perception using video stimuli and is potentially important given the rapid increase in the use of audiovisual videos in cognitive neuroscience research in recent years.},
  keywords     = {Temporal perception, Frame rate, Video quality, Speech, Audition, Vision},
}

@Article{Vroomen2010,
  author    = {Vroomen, Jean and Keetels, Mirjam},
  title     = {Perception of intersensory synchrony: A tutorial review},
  doi       = {10.3758/APP.72.4.871},
  pages     = {871--84},
  volume    = {72},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/vroomen2010.pdf:PDF},
  journal   = {Attention, Perception, \& Psychophysics},
  month     = may,
  owner     = {Aron Petau},
  timestamp = {2021-05-12},
  year      = {2010},
}

@Article{wang2018speaking,
  author    = {Wang, Mengyuan and Kong, Lingzhi and Zhang, Changxin and Wu, Xihong and Li, Liang},
  journal   = {The Journal of the Acoustical Society of America},
  title     = {Speaking rhythmically improves speech recognition under “cocktail-party” conditions},
  year      = {2018},
  issn      = {0001-4966},
  month     = apr,
  number    = {4},
  pages     = {EL255--EL259},
  volume    = {143},
  doi       = {10.1121/1.5030518},
  file      = {:wang2018.pdf:PDF},
  publisher = {Acoustical Society of America},
  timestamp = {2021-05-10},
}

@Article{youngkin2008,
  author    = {Younkin, Audrey C. and Corriveau, Philip J.},
  journal   = {IEEE Transactions on Broadcasting},
  title     = {Determining the Amount of Audio-Video Synchronization Errors Perceptible to the Average End-User},
  year      = {2008},
  issn      = {1557-9611},
  month     = sep,
  number    = {3},
  pages     = {623--627},
  volume    = {54},
  abstract  = {The media and acoustics perception lab (MAPL) designed a study to determine the minimum amount of audio-visual synchronization (a/v sync) errors that can be detected by end-users. Lip synchronization is the most noticeable a/v sync error, and was used as the testing stimuli to determine the perceptual threshold of audio leading errors. The results of the experiment determined that the average audio leading threshold for a/v sync detection was 185.19 ms, with a standard deviation of 42.32 ms. This threshold determination of lip sync error (with audio leading) will be widely used for validation and verification infrastructures across the industry. By implementing an objective pass/fail value into software, the system or network under test is held against criteria which were derived from a scientific subjective test.},
  doi       = {10.1109/TBC.2008.2002102},
  file      = {:younkin2008.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  timestamp = {2021-05-10},
}

@Article{Zakis2012,
  author    = {Justin A. Zakis and Bernadette Fulton and Brenton R. Steele},
  title     = {Preferred delay and phase-frequency response of open-canal hearing aids with music at low insertion gain},
  doi       = {10.3109/14992027.2012.701020},
  eprint    = {https://doi.org/10.3109/14992027.2012.701020},
  number    = {12},
  pages     = {906--913},
  url       = {https://doi.org/10.3109/14992027.2012.701020},
  volume    = {51},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/zakis2012.pdf:PDF},
  journal   = {International Journal of Audiology},
  owner     = {Aron Petau},
  publisher = {Taylor & Francis},
  timestamp = {2021-05-14},
  year      = {2012},
}

@Article{Zampini2003,
  author    = {Massimiliano Zampini and David I Shore and Charles Spence},
  journal   = {International Journal of Psychophysiology},
  title     = {Multisensory temporal order judgments: the role of hemispheric redundancy},
  year      = {2003},
  issn      = {0167-8760},
  month     = oct,
  note      = {Current findings in multisensory research},
  number    = {1},
  pages     = {165--180},
  volume    = {50},
  abstract  = {Participants made unspeeded ‘Which modality came first?’ temporal order judgments (TOJs) in response to pairs of auditory and visual stimuli presented at varying stimulus onset asynchronies (SOAs), using the method of constant stimuli. The presentation of auditory and visual stimuli from different spatial positions facilitated performance (i.e. just noticeable differences were lowered) only when the stimuli were presented across the body midline (Experiment 4), but not when both stimuli were either placed on the body midline (Experiments 1–3), or else within the same hemifield (Experiment 5). These results demonstrate that hemispheric redundancy may account for the facilitatory effects reported in previous multisensory TOJs research when stimuli were presented from different spatial locations. Our results also show that the accuracy with which people can make multisensory TOJs is unaffected by the predictability of target stimulus locations, suggesting little role for spatial attention in this aspect of multisensory temporal perception.},
  doi       = {https://doi.org/10.1016/S0167-8760(03)00132-6},
  file      = {:zampini2003.pdf:PDF},
  keywords  = {Multisensory perception, Temporal order judgments},
  owner     = {Aron Petau},
  publisher = {Elsevier {BV}},
  timestamp = {2021-05-06},
  url       = {https://www.sciencedirect.com/science/article/pii/S0167876003001326},
}

@Article{Zhou2020,
  author       = {Han-yu Zhou and Eric F.C. Cheung and Raymond C.K. Chan},
  date         = {2020},
  journaltitle = {Neuropsychologia},
  title        = {Audiovisual temporal integration: Cognitive processing, neural mechanisms, developmental trajectory and potential interventions},
  doi          = {https://doi.org/10.1016/j.neuropsychologia.2020.107396},
  issn         = {0028-3932},
  pages        = {107396},
  url          = {https://www.sciencedirect.com/science/article/pii/S0028393220300683},
  volume       = {140},
  abstract     = {To integrate auditory and visual signals into a unified percept, the paired stimuli must co-occur within a limited time window known as the Temporal Binding Window (TBW). The width of the TBW, a proxy of audiovisual temporal integration ability, has been found to be correlated with higher-order cognitive and social functions. A comprehensive review of studies investigating audiovisual TBW reveals several findings: (1) a wide range of top-down processes and bottom-up features can modulate the width of the TBW, facilitating adaptation to the changing and multisensory external environment; (2) a large-scale brain network works in coordination to ensure successful detection of audiovisual (a)synchrony; (3) developmentally, audiovisual TBW follows a U-shaped pattern across the lifespan, with a protracted developmental course into late adolescence and rebounding in size again in late life; (4) an enlarged TBW is characteristic of a number of neurodevelopmental disorders; and (5) the TBW is highly flexible via perceptual and musical training. Interventions targeting the TBW may be able to improve multisensory function and ameliorate social communicative symptoms in clinical populations.},
  file         = {:zhou2020.pdf:PDF},
  keywords     = {Temporal binding window, Cognitive processing, Neural mechanisms, Developmental trajectories, Neurodevelopmental disorders, Perceptual and music training},
  owner        = {aron},
  timestamp    = {2021-05-14},
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: fileDirectoryLatex-Aron Petau-AronLaptop:C:\\Users\\aron\\Google Drive\\BA\\thesis\\backups;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Materials\;0\;0\;0x8a8a8aff\;\;all the tools and software used\;;
1 StaticGroup:Multisensory Integration\;0\;1\;0x8a8a8aff\;\;\;;
}

@Comment{jabref-meta: saveActions:enabled;
all-text-fields[identity]
date[normalize_date]
month[normalize_month]
pages[normalize_page_numbers]
;}

@Comment{jabref-meta: saveOrderConfig:specified;citationkey;false;author;false;title;true;}
