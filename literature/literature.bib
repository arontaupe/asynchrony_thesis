% Encoding: UTF-8
Key:

@Article{rosemann2018audio,
  author    = {Rosemann, Stephanie and Thiel, Christiane M},
  journal   = {NeuroImage},
  title     = {Audio-visual speech processing in age-related hearing loss: Stronger integration and increased frontal lobe recruitment},
  year      = {2018},
  issn      = {1053-8119},
  pages     = {425--437},
  volume    = {175},
  doi       = {10.1016/j.neuroimage.2018.04.023},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/RosemannThiel2018NeuroImage.pdf:PDF},
  groups    = {Multisensory Integration, Age Effects},
  publisher = {Elsevier},
  timestamp = {2020-11-30},
}
Key : classical study on multisensory integration

@Article{nakamura2002integration,
  author    = {S. Nakamura},
  journal   = {IEEE Transactions on Neural Networks},
  title     = {Statistical multimodal integration for audio-visual speech processing},
  year      = {2002},
  issn      = {1045-9227},
  number    = {4},
  pages     = {854--866},
  volume    = {13},
  doi       = {10.1109/tnn.2002.1021886},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/nakamura2002.pdf:PDF},
  groups    = {Multisensory Integration},
  timestamp = {2020-11-30},
}

Key : handedness test

@Article{oldfield1971handedness,
  author    = {R.C. Oldfield},
  journal   = {Neuropsychologia},
  title     = {The assessment and analysis of handedness: The Edinburgh inventory},
  year      = {1971},
  issn      = {0028-3932},
  number    = {1},
  pages     = {97--113},
  volume    = {9},
  abstract  = {The need for a simply applied quantitative assessment of handedness is discussed and some previous forms reviewed. An inventory of 20 items with a set of instructions and response- and computational-conventions is proposed and the results obtained from a young adult population numbering some 1100 individuals are reported. The separate items are examined from the point of view of sex, cultural and socio-economic factors which might appertain to them and also of their inter-relationship to each other and to the measure computed from them all. Criteria derived from these considerations are then applied to eliminate 10 of the original 20 items and the results recomputed to provide frequency-distribution and cumulative frequency functions and a revised item-analysis. The difference of incidence of handedness between the sexes is discussed.},
  doi       = {https://doi.org/10.1016/0028-3932(71)90067-4},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/oldfield1971handedness.pdf:PDF;:C\:/Users/aron/Google Drive/BA/thesis/literature/beker2018.pdf:PDF;:C\:/Users/aron/Google Drive/BA/thesis/literature/beker2018.pdf:PDF},
  groups    = {Old Classics},
  timestamp = {2020-11-30},
  url       = {http://www.sciencedirect.com/science/article/pii/0028393271900674},
}

@Article{uslar2013stimuli,
  author    = {Uslar,Verena N. and Carroll,Rebecca and Hanke,Mirko and Hamann,Cornelia and Ruigendijk,Esther and Brand,Thomas and Kollmeier,Birger},
  journal   = {The Journal of the Acoustical Society of America},
  title     = {Development and evaluation of a linguistically and audiologically controlled sentence intelligibility test},
  year      = {2013},
  month     = oct,
  number    = {4},
  pages     = {3039--3056},
  volume    = {134},
  doi       = {10.1121/1.4818760},
  eprint    = {https://doi.org/10.1121/1.4818760},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/uslar2013OLAKS.pdf:PDF},
  groups    = {Materials},
  publisher = {Acoustical Society of America ({ASA})},
  timestamp = {2020-11-30},
  url       = {https://doi.org/10.1121/1.4818760},
}

@Article{Macdonald1978mcgurkeffect,
  author                  = {Macdonald, J. and McGurk, H.},
  journal                 = {Perception \& Psychophysics},
  title                   = {Visual influences on speech perception processes},
  year                    = {1978},
  issn                    = {00315117},
  note                    = {cited By 284},
  number                  = {3},
  pages                   = {253--257},
  volume                  = {24},
  abbrev_source_title     = {Percept. Psychophys.},
  abstract                = {An experiment is reported, the results of which confirm and extend an earlier observation that visual information for the speaker's lip movements profoundly modifies the auditorv perception of natural speech by normally hearing subjects. The effect is most pronounced when there is auditory information for a bilabial utterance combined with visual information for a nonlabial utterance. However, the effect is also obtained with the reverse combination, although to a lesser extent. These findings are considered for their relevance to auditory theories of speech perception. © 1978 Psychonomic Society, Inc.},
  affiliation             = {Department of Psychology, University of Surrey, Guildford, GU2 5XH, Surrey, United Kingdom},
  coden                   = {PEPSB},
  correspondence_address1 = {McGurk, H.; Department of Psychology, University of Surrey, Guildford, GU2 5XH, Surrey, United Kingdom},
  document_type           = {Article},
  doi                     = {10.3758/BF03206096},
  file                    = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Macdonald-McGurk1978VisualInfluencesOnSpeechPercep.pdf:PDF},
  groups                  = {Old Classics, Multisensory Integration},
  keywords                = {auditory system; central nervous system; human cell; normal human; visual system, Adolescent; Adult; Female; Human; Lipreading; Male; Phonetics; Speech Perception; Visual Perception},
  language                = {English},
  pubmed_id               = {704285},
  source                  = {Scopus},
  timestamp               = {2020-11-30},
  url                     = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018388657&doi=10.3758%2fBF03206096&partnerID=40&md5=c6d275c9b9a1362aeacf87e25a494621},
}

@Article{lezzoum2016threshold,
  author    = {Narimene Lezzoum and Ghyslain Gagnon and Jérémie Voix},
  journal   = {International Journal of Industrial Ergonomics},
  title     = {Echo threshold between passive and electro-acoustic transmission paths in digital hearing protection devices},
  year      = {2016},
  issn      = {0169-8141},
  pages     = {372--379},
  volume    = {53},
  abstract  = {Electronic hearing protection devices are increasingly used in noisy environments. Theses devices feature a miniaturized external microphone and internal loudspeaker in addition to an analog or digital electronic circuit. They can transmit useful audio signals such as speech and warning signals to the protected ear and can reduce the sound pressure level using dynamic range compression. In the case of a digital electronic circuit, the transmission of audio signals may be noticeably delayed because of the latency introduced by the digital signal processor and by the analog-to-digital and digital-to-analog converters. These delayed audio signals will hence interfere with the audio signals perceived naturally through the passive acoustical path of the device. The proposed study presents an original procedure to evaluate, for two representative passive earplugs, the shortest delay at which human listeners start to perceive two sounds composed of the signal transmitted through the electronic circuit and the passively transmitted signal. This shortest delay is called the echo threshold and represents the delay between the time of perception of one fused sound from two separate sounds. In this study, a transient signal, a clean speech signal, a speech signal corrupted by factory noise, and a speech signal corrupted by babble noise are used to determine the echo thresholds of the two earplugs. Twenty untrained listeners participated in this study, and were asked to determine the echo thresholds using a test software in which attenuated signals are delayed from the original signals in real-time. The findings show that when using hearing devices, the echo threshold depends on four parameters: (a) the attenuation function of the device, (b) the duration of the signal, (c) the level of the background noise and (d) the type of background noise. Defined here as the shortest time delay at which at least 20% of the participants noticed an echo, the echo threshold was found to be 8 ms for a bell signal, 16 ms for clean speech and 22 ms for speech corrupted by babble noise when using a shallow earplug fit. When using a deep fit, the echo threshold was found to be 18 ms for a bell signal and 26 ms for clean speech and 68 ms for speech in factory. No echo threshold could be clearly determined for the speech signal in babble noise with a deep earplug fit.},
  doi       = {https://doi.org/10.1016/j.ergon.2016.04.004},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Lezzoum2016EchoThreshold between Passive and Electro-Acousti.pdf:PDF},
  groups    = {Echo Effect, Delay},
  keywords  = {Hearing protection device, Echo threshold, Just noticeable difference, Delay},
  timestamp = {2020-11-30},
  url       = {http://www.sciencedirect.com/science/article/pii/S0169814116300233},
}

@Article{noel2017atypical,
  author    = {Noel, Jean-Paul and De Niear, Matthew A and Stevenson, Ryan and Alais, David and Wallace, Mark T},
  journal   = {Autism Research},
  title     = {Atypical rapid audio-visual temporal recalibration in autism spectrum disorders},
  year      = {2017},
  number    = {1},
  pages     = {121--129},
  volume    = {10},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Noel2017 Atypical rapid audio-visual temporal recalibration.pdf:PDF},
  groups    = {Autism, Multisensory Integration},
  publisher = {Wiley Online Library},
  timestamp = {2020-11-30},
}

@Article{stone2002tolerable,
  author    = {Stone, Michael A and Moore, Brian CJ},
  journal   = {Ear and Hearing},
  title     = {Tolerable hearing aid delays. II. Estimation of limits imposed during speech production},
  year      = {2002},
  issn      = {0196-0202},
  number    = {4},
  pages     = {325--338},
  volume    = {23},
  doi       = {10.1097/00003446-200208000-00008},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Stone2002TolerableDelays.pdf:PDF},
  groups    = {Delay},
  publisher = {LWW},
  timestamp = {2020-11-30},
}

@Article{bosker2020visual,
  author    = {Bosker, Hans Rutger and Peeters, David and Holler, Judith},
  journal   = {Quarterly Journal of Experimental Psychology},
  title     = {How visual cues to speech rate influence speech perception},
  year      = {2020},
  issn      = {1747-0218},
  pages     = {1747021820914564},
  volume    = {73},
  doi       = {10.1177/1747021820914564},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Bosker2020_lexical selection_audiovisual.pdf:PDF},
  groups    = {Multisensory Integration, Unused},
  publisher = {SAGE Publications Sage UK: London, England},
  timestamp = {2020-11-30},
}

@Article{Maier2011asynchrony,
  author    = {Maier, Joost and Di Luca, Massimiliano and Noppeney, Uta},
  journal   = {Journal of experimental psychology. Human perception and performance},
  title     = {Audiovisual Asynchrony Detection in Human Speech},
  year      = {2011},
  month     = feb,
  pages     = {245--56},
  volume    = {37},
  doi       = {10.1037/a0019952},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Maier2011-audiovisual-asynchrony-speech.pdf:PDF},
  groups    = {Delay},
  timestamp = {2020-11-30},
}

@Article{soto2004assessing,
  author    = {Soto-Faraco, Salvador and Navarra, Jordi and Alsius, Agnes},
  journal   = {Cognition},
  title     = {Assessing automaticity in audiovisual speech integration: evidence from the speeded classification task},
  year      = {2004},
  issn      = {0010-0277},
  number    = {3},
  pages     = {B13--B23},
  volume    = {92},
  comment   = {Nice easy and short, provides evidence for automaticity of multimodal integration via an indirect mcGurk Effect to induce perceived inconsistency, resulting in slower reaction times},
  doi       = {10.1016/j.cognition.2003.10.005},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/soto-faraco2004-automaticity-audiovisual-integration.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Elsevier},
  timestamp = {2020-11-30},
}

@Article{ross2007you,
  author    = {Ross, Lars A and Saint-Amour, Dave and Leavitt, Victoria M and Javitt, Daniel C and Foxe, John J},
  journal   = {Cerebral cortex},
  title     = {Do you see what I am saying? Exploring visual enhancement of speech comprehension in noisy environments},
  year      = {2007},
  issn      = {0920-9964},
  number    = {5},
  pages     = {1147--1153},
  volume    = {17},
  doi       = {10.1016/j.schres.2007.08.008},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/ross2006-noisy.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Oxford University Press},
  timestamp = {2020-11-30},
}

@Book{stein1993merging,
  author    = {Stein, Barry E and Meredith, M Alex},
  publisher = {The MIT Press},
  title     = {The merging of the senses.},
  year      = {1993},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/stein1993.pdf:PDF},
  groups    = {Multisensory Integration, Old Classics},
  timestamp = {2020-11-30},
}

@Article{meredith1986visual,
  author    = {Meredith, M Alex and Stein, Barry E},
  journal   = {Journal of neurophysiology},
  title     = {Visual, auditory, and somatosensory convergence on cells in superior colliculus results in multisensory integration},
  year      = {1986},
  issn      = {0022-3077},
  number    = {3},
  pages     = {640--662},
  volume    = {56},
  doi       = {10.1152/jn.1986.56.3.640},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/meredith1986.pdf:PDF},
  groups    = {Multisensory Integration, Old Classics},
  publisher = {American Physiological Society Bethesda, MD},
  timestamp = {2020-11-30},
}

@InCollection{stein1993integration,
  author    = {Barry E. Stein and M. Alex Meredith and Mark T. Wallace},
  publisher = {Elsevier},
  title     = {Chapter 8 The visually responsive neuron and beyond: multisensory integration in cat and monkey},
  year      = {1993},
  editor    = {T.P. Hicks and S. Molotchnikoff and T. Ono},
  pages     = {79--90},
  series    = {Progress in Brain Research},
  volume    = {95},
  abstract  = {Publisher Summary
The cat superior colliculus neuron is a major site for the convergence and integration of multisensory information. It is only one of many central nervous system sites in many species where information from several modalities converges. Single cells in cat LS (lateral suprasylvian) and AES (anterior ectosylvian sulcus) and in monkey IPS (intraparietal sulcus) and STS (superior temporal sulcus) were examined. Although the spatial, temporal, and multiplicative characteristics of multisensory integration were most closely examined in cat cortex, all of the observations in monkey were consistent with those described in cat. The multisensory receptive fields of a single neuron overlapped one another in space, such that sensory stimuli that were in close spatial register fell within their excitatory receptive fields and enhanced the neuron's activity; spatially disparate stimuli produced either no interaction or depressed responses. The rules of multisensory integration evident at the level of the single neuron are also consistent with studies of intact behaving animals. The attentive and orientation responses cats make to visual and auditory stimuli were predictable based on the reactions of superior colliculus neurons to these stimuli. The data indicate that the visual responses of many neurons, whether in the superior colliculus or cortex, represent only one facet of their sensory coding capabilities.},
  doi       = {https://doi.org/10.1016/S0079-6123(08)60359-3},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/stein1993.pdf:PDF},
  groups    = {Multisensory Integration, Unused},
  issn      = {0079-6123},
  timestamp = {2020-11-30},
  url       = {http://www.sciencedirect.com/science/article/pii/S0079612308603593},
}

@Article{vanwassenhove2007window,
  author    = {Virginie {van Wassenhove} and Ken W. Grant and David Poeppel},
  journal   = {Neuropsychologia},
  title     = {Temporal window of integration in auditory-visual speech perception},
  year      = {2007},
  issn      = {0028-3932},
  note      = {Advances in Multisensory Processes},
  number    = {3},
  pages     = {598--607},
  volume    = {45},
  abstract  = {Forty-three normal hearing participants were tested in two experiments, which focused on temporal coincidence in auditory visual (AV) speech perception. In these experiments, audio recordings of/pa/and/ba/were dubbed onto video recordings of /ba/or/ga/, respectively (ApVk, AbVg), to produce the illusory “fusion” percepts /ta/, or /da/ [McGurk, H., & McDonald, J. (1976). Hearing lips and seeing voices. Nature, 264, 746–747]. In Experiment 1, an identification task using McGurk pairs with asynchronies ranging from −467ms (auditory lead) to +467ms was conducted. Fusion responses were prevalent over temporal asynchronies from −30ms to +170ms and more robust for audio lags. In Experiment 2, simultaneity judgments for incongruent and congruent audiovisual tokens (AdVd, AtVt) were collected. McGurk pairs were more readily judged as asynchronous than congruent pairs. Characteristics of the temporal window over which simultaneity and fusion responses were maximal were quite similar, suggesting the existence of a 200ms duration asymmetric bimodal temporal integration window.},
  doi       = {https://doi.org/10.1016/j.neuropsychologia.2006.01.001},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/vanwassenhove2007temporalWindow.pdf:PDF},
  groups    = {Multisensory Integration, Delay},
  keywords  = {McGurk illusion, Multisensory, Time, Psychophysics, Analysis-by-synthesis},
  timestamp = {2020-11-30},
  url       = {http://www.sciencedirect.com/science/article/pii/S002839320600011X},
}

@Misc{audacity,
  author    = {Audacity},
  title     = {Audacity® software is copyright © 1999-2020 Audacity Team.Web site: https://audacityteam.org/. It is free softwaredistributed under the terms of the GNU General Public License.The name Audacity® is a registered trademark of Dominic Mazzoni.},
  year      = {2020},
  groups    = {Materials},
  timestamp = {2020-11-30},
}

@Manual{rstudio,
  title        = {RStudio: Integrated Development Environment for R},
  address      = {Boston, MA},
  author       = {{RStudio Team}},
  organization = {RStudio, PBC.},
  year         = {2020},
  groups       = {Materials},
  timestamp    = {2020-11-30},
  url          = {http://www.rstudio.com/},
}

@Article{quene2007justnoticeabledifference,
  author    = {Hugo Quené},
  journal   = {Journal of Phonetics},
  title     = {On the just noticeable difference for tempo in speech},
  year      = {2007},
  issn      = {0095-4470},
  number    = {3},
  pages     = {353--362},
  volume    = {35},
  abstract  = {Speakers vary their speech tempo (speaking rate), and such variations in tempo are quite noticeable. But what is the just noticeable difference (JND) for tempo in speech? The present study aims at providing a realistic and robust estimate, by using multiple speech tokens from multiple speakers. The JND is assessed in two (2IAX and 2IFC) comparison experiments, yielding an estimated JND for speech tempo of about 5%. A control experiment suggests that this finding is not due to acoustic artefacts of the tempo-transformation method used. Tempo variations within speakers typically exceed this JND, which makes such variations relevant in speech communication.},
  doi       = {https://doi.org/10.1016/j.wocn.2006.09.001},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/quene2007justnoticeabledifference.pdf:PDF},
  groups    = {Delay, Multisensory Integration},
  timestamp = {2020-11-30},
  url       = {http://www.sciencedirect.com/science/article/pii/S0095447006000441},
}

@Article{bertelson2003visual,
  author    = {Bertelson, Paul and Vroomen, Jean and De Gelder, B{\'e}atrice},
  journal   = {Psychological Science},
  title     = {Visual recalibration of auditory speech identification: a McGurk aftereffect},
  year      = {2003},
  issn      = {0956-7976},
  number    = {6},
  pages     = {592--597},
  volume    = {14},
  doi       = {10.1046/j.0956-7976.2003.psci_1470.x},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/bertelson2003recalibration.pdf:PDF},
  groups    = {Multisensory Integration, Delay},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  subtitle  = {A McGurk Aftereffect},
  timestamp = {2020-11-30},
}

@Article{biau2015speaker,
  author    = {Biau, Emmanuel and Torralba, Mireia and Fuentemilla, Lluis and de Diego Balaguer, Ruth and Soto-Faraco, Salvador},
  journal   = {Cortex},
  title     = {Speaker's hand gestures modulate speech perception through phase resetting of ongoing neural oscillations},
  year      = {2015},
  issn      = {0010-9452},
  pages     = {76--85},
  volume    = {68},
  doi       = {10.1016/j.cortex.2014.11.018},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/biau2015handGestures.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Elsevier},
  timestamp = {2020-11-30},
}

@Article{calvert1997activation,
  author    = {Calvert, Gemma A and Bullmore, Edward T and Brammer, Michael J and Campbell, Ruth and Williams, Steven CR and McGuire, Philip K and Woodruff, Peter WR and Iversen, Susan D and David, Anthony S},
  journal   = {science},
  title     = {Activation of auditory cortex during silent lipreading},
  year      = {1997},
  number    = {5312},
  pages     = {593--596},
  volume    = {276},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/calvert1997auditoryActivation.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2020-11-30},
}

@Article{crosse2015congruent,
  author    = {Crosse, Michael J and Butler, John S and Lalor, Edmund C},
  journal   = {Journal of Neuroscience},
  title     = {Congruent visual speech enhances cortical entrainment to continuous auditory speech in noise-free conditions},
  year      = {2015},
  issn      = {0270-6474},
  number    = {42},
  pages     = {14195--14204},
  volume    = {35},
  doi       = {10.1523/jneurosci.1829-15.2015},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/crosse2015congruent.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Soc Neuroscience},
  timestamp = {2020-11-30},
}

@Article{iversen2015synchronization,
  author    = {Iversen, John R and Patel, Aniruddh D and Nicodemus, Brenda and Emmorey, Karen},
  journal   = {Cognition},
  title     = {Synchronization to auditory and visual rhythms in hearing and deaf individuals},
  year      = {2015},
  issn      = {0010-0277},
  pages     = {232--244},
  volume    = {134},
  doi       = {10.1016/j.cognition.2014.10.018},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/iversen2015synchronization.pdf:PDF},
  groups    = {Multisensory Integration},
  publisher = {Elsevier},
  timestamp = {2020-11-30},
}

@InCollection{rosenblum2019audiovisual,
  author    = {Rosenblum, Lawrence D},
  booktitle = {Oxford Research Encyclopedia of Linguistics},
  title     = {Audiovisual speech perception and the McGurk effect},
  year      = {2019},
  doi       = {10.1093/acrefore/9780199384655.013.420},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/rosenblum2019audiovisual.pdf:PDF},
  groups    = {Multisensory Integration},
  timestamp = {2020-11-30},
}

@Misc{puredata,
  author    = {puredata},
  title     = {puredata.info},
  year      = {2020},
  groups    = {Materials},
  timestamp = {2020-11-30},
}

@Article{pouw2019entrainment,
  author    = {Pouw, Wim and Dixon, James A},
  journal   = {Cognitive Science},
  title     = {Entrainment and modulation of gesture--speech synchrony under delayed auditory feedback},
  year      = {2019},
  issn      = {0364-0213},
  number    = {3},
  pages     = {e12721},
  volume    = {43},
  doi       = {10.1111/cogs.12721},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/pouw2019entrainment.pdf:PDF},
  groups    = {Delay},
  publisher = {Wiley Online Library},
  timestamp = {2020-11-30},
}

@Article{esteve2018prosody,
  author    = {Esteve-Gibert, N{\'u}ria and Guella{\"\i}, Bahia},
  journal   = {Frontiers in Psychology},
  title     = {Prosody in the auditory and visual domains: A developmental perspective},
  year      = {2018},
  issn      = {1664-1078},
  pages     = {338},
  volume    = {9},
  doi       = {10.3389/fpsyg.2018.00338},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/esteve2018prosody.pdf:PDF},
  groups    = {Language Development, Unused},
  publisher = {Frontiers},
  timestamp = {2020-11-30},
}

@Article{Hay-McCutcheon2009Asynchrony,
  author    = {Marcia J. Hay-McCutcheon and David B. Pisoni and Kristopher K. Hunt},
  journal   = {International Journal of Audiology},
  title     = {Audiovisual asynchrony detection and speech perception in hearing-impaired listeners with cochlear implants: A preliminary analysis},
  year      = {2009},
  number    = {6},
  pages     = {321--333},
  volume    = {48},
  abstract  = {This preliminary study examined the effects of hearing loss and aging on the detection of AV asynchrony in hearing-impaired listeners with cochlear implants. Additionally, the relationship between AV asynchrony detection skills and speech perception was assessed. Individuals with normal-hearing and cochlear implant recipients were asked to make judgments about the synchrony of AV speech. The cochlear implant recipients also completed three speech perception tests, the CUNY, HINT sentences, and the CNC test. No significant differences were observed in the detection of AV asynchronous speech between the normal-hearing listeners and the cochlear implant recipients. Older adults in both groups displayed wider timing windows, over which they identified AV asynchronous speech as being synchronous, than younger adults. For the cochlear implant recipients, no relationship between the size of the temporal asynchrony window and speech perception performance was observed. The findings from this preliminary experiment suggest that aging has a greater effect on the detection of AV asynchronous speech than the use of a cochlear implant. Additionally, the temporal width of the AV asynchrony function was not correlated with speech perception skills for hearing-impaired individuals who use cochlear implants.},
  doi       = {10.1080/14992020802644871},
  eprint    = {https://doi.org/10.1080/14992020802644871},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/Hay-McCutcheon2009Asynchrony.pdf:PDF},
  groups    = {Age Effects, Hearing-Impaired},
  publisher = {Taylor & Francis},
  timestamp = {2020-11-30},
  url       = {https://doi.org/10.1080/14992020802644871},
}

@Article{turi2016noRecalibrationAutism,
  author    = {Turi, Marco and Karaminis, Themelis and Pellicano, Elizabeth and Burr, David},
  journal   = {Scientific reports},
  title     = {No rapid audiovisual recalibration in adults on the autism spectrum},
  year      = {2016},
  issn      = {2045-2322},
  pages     = {21756},
  volume    = {6},
  doi       = {10.1038/srep21756},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/turi2016noRecalibrationAutism.pdf:PDF},
  groups    = {Autism, Multisensory Integration},
  publisher = {Nature Publishing Group},
  timestamp = {2020-11-30},
}

@InProceedings{ren2016asynchrony,
  author    = {Y. {Ren} and W. {Yang} and Q. {Wu} and F. {Wu} and S. {Takahashi} and E. {Yoshimichi} and J. {Wu}},
  booktitle = {2016 IEEE International Conference on Mechatronics and Automation},
  title     = {Study of audiovisual asynchrony signal processing: Robot recognition system of different ages},
  year      = {2016},
  pages     = {2320--2325},
  doi       = {10.1109/ICMA.2016.7558927},
  file      = {:C\:/Users/Aron Petau/Google Drive/BA/thesis/literature/ren2016asynchrony.pdf:PDF},
  groups    = {Age Effects, Unused},
  timestamp = {2020-11-30},
}

@Article{goehring2018tolerable,
  author    = {Goehring, Tobias and Chapman, Josie L and Bleeck, Stefan and Monaghan*, Jessica JM},
  journal   = {International journal of audiology},
  title     = {Tolerable delay for speech production and perception: effects of hearing ability and experience with hearing aids},
  year      = {2018},
  number    = {1},
  pages     = {61--68},
  volume    = {57},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/goehring2017.pdf:PDF},
  groups    = {Delay, Hearing-Impaired, Unused},
  publisher = {Taylor \& Francis},
  timestamp = {2020-11-30},
}

@Article{giordano2017,
  author       = {Giordano, Bruno L and Ince, Robin A A and Gross, Joachim and Schyns, Philippe G and Panzeri, Stefano and Kayser, Christoph},
  journal      = {eLife},
  title        = {Contributions of local speech encoding and functional connectivity to audio-visual speech perception},
  year         = {2017},
  issn         = {2050-084X},
  month        = jun,
  pages        = {e24763},
  volume       = {6},
  abstract     = {Seeing a speaker’s face enhances speech intelligibility in adverse environments. We investigated the underlying network mechanisms by quantifying local speech representations and directed connectivity in MEG data obtained while human participants listened to speech of varying acoustic SNR and visual context. During high acoustic SNR speech encoding by temporally entrained brain activity was strong in temporal and inferior frontal cortex, while during low SNR strong entrainment emerged in premotor and superior frontal cortex. These changes in local encoding were accompanied by changes in directed connectivity along the ventral stream and the auditory-premotor axis. Importantly, the behavioral benefit arising from seeing the speaker’s face was not predicted by changes in local encoding but rather by enhanced functional connectivity between temporal and inferior frontal cortex. Our results demonstrate a role of auditory-frontal interactions in visual speech representations and suggest that functional connectivity along the ventral pathway facilitates speech comprehension in multisensory environments.},
  article_type = {journal},
  citation     = {eLife 2017;6:e24763},
  doi          = {10.7554/eLife.24763},
  editor       = {Schroeder, Charles E},
  file         = {:C\:/Users/aron/Google Drive/BA/thesis/literature/giordano2017localspeechencoding.pdf:PDF},
  groups       = {Multisensory Integration, Perception Enhancement, Unused},
  keywords     = {auditory system, magnetoencephalography, audio-visual speech entrainment, directed functional connectivity, inferior frontal gyrus, premotor cortex},
  pub_date     = {2017-06-07},
  publisher    = {eLife Sciences Publications, Ltd},
  timestamp    = {2020-11-30},
  url          = {https://doi.org/10.7554/eLife.24763},
}

@Article{anderson2017plasticity,
  author    = {Carly A. Anderson and Diane S. Lazard and Douglas E.H. Hartley},
  journal   = {Hearing Research},
  title     = {Plasticity in bilateral superior temporal cortex: Effects of deafness and cochlear implantation on auditory and visual speech processing},
  year      = {2017},
  issn      = {0378-5955},
  note      = {Plasticity Following Hearing Loss and Deafness},
  pages     = {138--149},
  volume    = {343},
  abstract  = {While many individuals can benefit substantially from cochlear implantation, the ability to perceive and understand auditory speech with a cochlear implant (CI) remains highly variable amongst adult recipients. Importantly, auditory performance with a CI cannot be reliably predicted based solely on routinely obtained information regarding clinical characteristics of the CI candidate. This review argues that central factors, notably cortical function and plasticity, should also be considered as important contributors to the observed individual variability in CI outcome. Superior temporal cortex (STC), including auditory association areas, plays a crucial role in the processing of auditory and visual speech information. The current review considers evidence of cortical plasticity within bilateral STC, and how these effects may explain variability in CI outcome. Furthermore, evidence of audio-visual interactions in temporal and occipital cortices is examined, and relation to CI outcome is discussed. To date, longitudinal examination of changes in cortical function and plasticity over the period of rehabilitation with a CI has been restricted by methodological challenges. The application of functional near-infrared spectroscopy (fNIRS) in studying cortical function in CI users is becoming increasingly recognised as a potential solution to these problems. Here we suggest that fNIRS offers a powerful neuroimaging tool to elucidate the relationship between audio-visual interactions, cortical plasticity during deafness and following cochlear implantation, and individual variability in auditory performance with a CI.},
  doi       = {https://doi.org/10.1016/j.heares.2016.07.013},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/anderson2017.pdf:PDF},
  groups    = {Hearing-Impaired, Unused},
  keywords  = {Audio-visual interactions, Cortical plasticity, Functional near-infrared spectroscopy, Occipital cortex, Speechreading, Superior temporal cortex},
  timestamp = {2020-11-30},
  url       = {http://www.sciencedirect.com/science/article/pii/S0378595516301174},
}

@Article{sakamoto2018,
  author    = {Shuichi Sakamoto and Zhenglie Cui and Tomori Miyashita and Masayuki Morimoto and Yôiti Suzuki and Hayato Sato},
  journal   = {Applied Acoustics},
  title     = {Effects of inter-word pauses on speech intelligibility under long-path echo conditions},
  year      = {2018},
  issn      = {0003-682X},
  pages     = {263--274},
  volume    = {140},
  abstract  = {Long-path echo is a salient factor that causes the degradation of the intelligibility of speech transmitted through a wide area outdoor environment or a very large indoor space using public-address systems. To robustly transmit speech information under such conditions, it is important to overcome this effect by controlling the characteristics of speech sounds. In this study, we consider the effects of inserting pauses between the words of a sentence. We performed word intelligibility tests using a series of four continuous words, called a quadruplet. Various pause lengths and long-path echo patterns were applied to the quadruplet. The results of the experiments demonstrate that word intelligibility under a long-path echo is significantly improved by the insertion of pauses between the words. Intelligibility can approach the same levels observed in the absence of echoes for a pause length of approximately 200 ms, which is almost the same as the length of 1-mora for the words used in the experiments. Moreover, this 200 ms pause is known to be sufficient to improve speech recognition in older adults. These results suggest that inter-word pauses of a length of approximately 1-mora can generally enhance the robustness of speech communication systems when used under a severe environment.},
  doi       = {https://doi.org/10.1016/j.apacoust.2018.01.020},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/sakamoto2018.pdf:PDF},
  groups    = {Perception Enhancement, Echo Effect, Unused},
  keywords  = {Word intelligibility, Long-path echo, Pause, Open-air public-address systems},
  timestamp = {2020-11-30},
  url       = {http://www.sciencedirect.com/science/article/pii/S0003682X17310575},
}

@Article{stilp2020,
  author    = {Stilp, Christian},
  journal   = {WIREs Cognitive Science},
  title     = {Acoustic context effects in speech perception},
  year      = {2020},
  number    = {1},
  pages     = {e1517},
  volume    = {11},
  abstract  = {Abstract The extreme acoustic variability of speech is well established, which makes the proficiency of human speech perception all the more impressive. Speech perception, like perception in any modality, is relative to context, and this provides a means to normalize the acoustic variability in the speech signal. Acoustic context effects in speech perception have been widely documented, but a clear understanding of how these effects relate to each other across stimuli, timescales, and acoustic domains is lacking. Here we review the influences that spectral context, temporal context, and spectrotemporal context have on speech perception. Studies are organized in terms of whether the context precedes the target (forward effects) or follows it (backward effects), and whether the context is adjacent to the target (proximal) or temporally removed from it (distal). Special cases where proximal and distal contexts have competing influences on perception are also considered. Across studies, a common theme emerges: acoustic differences between contexts and targets are perceptually magnified, producing contrast effects that facilitate perception of target sounds and words. This indicates enhanced sensitivity to changes in the acoustic environment, which maximizes the amount of potential information that can be transmitted to the perceiver. This article is categorized under: Linguistics > Language in Mind and Brain Psychology > Perception and Psychophysics},
  doi       = {https://doi.org/10.1002/wcs.1517},
  eprint    = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.1517},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/stilp2019.pdf:PDF},
  groups    = {Echo Effect},
  keywords  = {context effects, speech categorization, speech perception},
  timestamp = {2020-11-30},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1517},
}

@Article{du2016increased,
  author    = {Du, Yi and Buchsbaum, Bradley R and Grady, Cheryl L and Alain, Claude},
  journal   = {Nature communications},
  title     = {Increased activity in frontal motor cortex compensates impaired speech perception in older adults},
  year      = {2016},
  issn      = {2041-1723},
  number    = {1},
  pages     = {1--12},
  volume    = {7},
  doi       = {10.1038/ncomms12241},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/du2016.pdf:PDF},
  groups    = {Age Effects, Hearing-Impaired},
  publisher = {Nature Publishing Group},
  timestamp = {2020-11-30},
}

@Article{klockgether2016,
  author    = {Klockgether,Stefan and van de Par,Steven},
  journal   = {The Journal of the Acoustical Society of America},
  title     = {Just noticeable differences of spatial cues in echoic and anechoic acoustical environments},
  year      = {2016},
  number    = {4},
  pages     = {EL352-EL357},
  volume    = {140},
  doi       = {10.1121/1.4964844},
  eprint    = {https://doi.org/10.1121/1.4964844},
  file      = {:C\:/Users/aron/Google Drive/BA/thesis/literature/klockgether2016.pdf:PDF},
  timestamp = {2020-11-30},
  url       = {https://doi.org/10.1121/1.4964844},
}

@Article{beker2018autismDevelopment,
  author    = {Shlomit Beker and John J. Foxe and Sophie Molholm},
  journal   = {Neuroscience & Biobehavioral Reviews},
  title     = {Ripe for solution: Delayed development of multisensory processing in autism and its remediation},
  year      = {2018},
  issn      = {0149-7634},
  pages     = {182--192},
  volume    = {84},
  abstract  = {Difficulty integrating inputs from different sensory sources is commonly reported in individuals with Autism Spectrum Disorder (ASD). Accumulating evidence consistently points to altered patterns of behavioral reactions and neural activity when individuals with ASD observe or act upon information arriving through multiple sensory systems. For example, impairments in the integration of seen and heard speech appear to be particularly acute, with obvious implications for interpersonal communication. Here, we explore the literature on multisensory processing in autism with a focus on developmental trajectories. While much remains to be understood, some consistent observations emerge. Broadly, sensory integration deficits are found in children with an ASD whereas these appear to be much ameliorated, or even fully recovered, in older teenagers and adults on the spectrum. This protracted delay in the development of multisensory processing raises the possibility of applying early intervention strategies focused on multisensory integration, to accelerate resolution of these functions. We also consider how dysfunctional cross-sensory oscillatory neural communication may be one key pathway to impaired multisensory processing in ASD.},
  doi       = {https://doi.org/10.1016/j.neubiorev.2017.11.008},
  groups    = {Unused},
  keywords  = {Autism spectrum disorder, Typical development, Multisensory integration, Sensory processing, Amelioration, Normalization, Time window of integration, Recovery, Oscillation, Phase alignment, Phase reset},
  timestamp = {2020-11-30},
  url       = {http://www.sciencedirect.com/science/article/pii/S0149763417305262},
}

@Article{brandwein2013development,
  author    = {Brandwein, Alice B and Foxe, John J and Butler, John S and Russo, Natalie N and Altschuler, Ted S and Gomes, Hilary and Molholm, Sophie},
  journal   = {Cerebral Cortex},
  title     = {The development of multisensory integration in high-functioning autism: high-density electrical mapping and psychophysical measures reveal impairments in the processing of audiovisual inputs},
  year      = {2013},
  number    = {6},
  pages     = {1329--1341},
  volume    = {23},
  file      = {:C\:/Users/Aron/Google Drive/BA/thesis/literature/brandwein2013highFunctioning.pdf:PDF},
  publisher = {Oxford University Press},
  timestamp = {2020-11-30},
}

@Article{stevenson2014impact,
  author    = {Stevenson, Ryan A and Segers, Magali and Ferber, Susanne and Barense, Morgan D and Wallace, Mark T},
  journal   = {Frontiers in psychology},
  title     = {The impact of multisensory integration deficits on speech perception in children with autism spectrum disorders},
  year      = {2014},
  pages     = {379},
  volume    = {5},
  file      = {:C\:/Users/Aron/Google Drive/BA/thesis/literature/stevenson2014autismDeficits.pdf:PDF},
  publisher = {Frontiers},
  timestamp = {2020-11-30},
}

@Book{american2013diagnostic,
  author    = {American Psychiatric Association APA},
  publisher = {American Psychiatric Association},
  title     = {Diagnostic and Statistical Manual of Mental Disorders},
  year      = {2013},
  month     = may,
  doi       = {10.1176/appi.books.9780890425596},
  timestamp = {2020-12-05},
}

@Article{pollack1954visual,
  author    = {Pollack, Sumby WH},
  journal   = {J Acoust Soc Am},
  title     = {I 1954 Visual contribution to speech intelligibility in noise},
  year      = {1954},
  pages     = {212215},
  volume    = {26},
  timestamp = {2020-11-30},
}

@Article{badian1979standardized,
  author    = {Badian, M and Appel, E and Palm, D and Rupp, W and Sittig, W and Taeuber, K},
  journal   = {European journal of clinical pharmacology},
  title     = {Standardized mental stress in healthy volunteers induced by delayed auditory feedback (DAF)},
  year      = {1979},
  number    = {3},
  pages     = {171--176},
  volume    = {16},
  publisher = {Springer},
  timestamp = {2020-11-30},
}

@Book{mcneill1992hand,
  author    = {McNeill, David},
  publisher = {University of Chicago press},
  title     = {Hand and mind: What gestures reveal about thought},
  year      = {1992},
  timestamp = {2020-11-30},
}

@Article{stratton1896some,
  author    = {Stratton, George M},
  journal   = {Psychological review},
  title     = {Some preliminary experiments on vision without inversion of the retinal image.},
  year      = {1896},
  number    = {6},
  pages     = {611},
  volume    = {3},
  publisher = {The Macmillan Company},
  timestamp = {2020-12-05},
}

@Article{bridges2020timing,
  author    = {Bridges, David and Pitiot, Alain and MacAskill, Michael R and Peirce, Jonathan W},
  journal   = {PeerJ},
  title     = {The timing mega-study: comparing a range of experiment generators, both lab-based and online},
  year      = {2020},
  month     = jan,
  pages     = {e9414},
  volume    = {8},
  doi       = {10.31234/osf.io/d6nu5},
  publisher = {PeerJ Inc.},
  timestamp = {2021-01-11},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Echo Effect\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Old Classics\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Multisensory Integration\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Delay\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Autism\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Materials\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Language Development\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Age Effects\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Hearing-Impaired\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Perception Enhancement\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Unused\;0\;1\;0x8a8a8aff\;\;\;;
}

@Comment{jabref-meta: saveActions:enabled;
all-text-fields[identity]
date[normalize_date]
month[normalize_month]
pages[normalize_page_numbers]
;}
