% Encoding: UTF-8
Key:
@article{rosemann2018audio,
  title={Audio-visual speech processing in age-related hearing loss: Stronger integration and increased frontal lobe recruitment},
  author={Rosemann, Stephanie and Thiel, Christiane M},
  journal={NeuroImage},
  volume={175},
  pages={425--437},
  year={2018},
  publisher={Elsevier}
}
Key : classical study on multisensory integration
@ARTICLE{nakamura2002integration,
  author={S. {Nakamura}},
  journal={IEEE Transactions on Neural Networks},
  title={Statistical multimodal integration for audio-visual speech processing},
  year={2002},
  volume={13},
  number={4},
  pages={854-866},
}

Key : handedness test
@article{oldfield1971handedness,
title = "The assessment and analysis of handedness: The Edinburgh inventory",
journal = "Neuropsychologia",
volume = "9",
number = "1",
pages = "97 - 113",
year = "1971",
issn = "0028-3932",
doi = "https://doi.org/10.1016/0028-3932(71)90067-4",
url = "http://www.sciencedirect.com/science/article/pii/0028393271900674",
author = "R.C. Oldfield",
abstract = "The need for a simply applied quantitative assessment of handedness is discussed and some previous forms reviewed. An inventory of 20 items with a set of instructions and response- and computational-conventions is proposed and the results obtained from a young adult population numbering some 1100 individuals are reported. The separate items are examined from the point of view of sex, cultural and socio-economic factors which might appertain to them and also of their inter-relationship to each other and to the measure computed from them all. Criteria derived from these considerations are then applied to eliminate 10 of the original 20 items and the results recomputed to provide frequency-distribution and cumulative frequency functions and a revised item-analysis. The difference of incidence of handedness between the sexes is discussed."
}

@Article{uslar2013stimuli,
  author  = {Uslar,Verena N. and Carroll,Rebecca and Hanke,Mirko and Hamann,Cornelia and Ruigendijk,Esther and Brand,Thomas and Kollmeier,Birger},
  journal = {The Journal of the Acoustical Society of America},
  title   = {Development and evaluation of a linguistically and audiologically controlled sentence intelligibility test},
  year    = {2013},
  number  = {4},
  pages   = {3039-3056},
  volume  = {134},
  doi     = {10.1121/1.4818760},
  eprint  = {https://doi.org/10.1121/1.4818760},
  url     = {https://doi.org/10.1121/1.4818760},
}

@ARTICLE{Macdonald1978mcgurkeffect,
author={Macdonald, J. and McGurk, H.},
title={Visual influences on speech perception processes},
journal={Perception & Psychophysics},
year={1978},
volume={24},
number={3},
pages={253-257},
doi={10.3758/BF03206096},
note={cited By 284},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018388657&doi=10.3758%2fBF03206096&partnerID=40&md5=c6d275c9b9a1362aeacf87e25a494621},
affiliation={Department of Psychology, University of Surrey, Guildford, GU2 5XH, Surrey, United Kingdom},
abstract={An experiment is reported, the results of which confirm and extend an earlier observation that visual information for the speaker's lip movements profoundly modifies the auditorv perception of natural speech by normally hearing subjects. The effect is most pronounced when there is auditory information for a bilabial utterance combined with visual information for a nonlabial utterance. However, the effect is also obtained with the reverse combination, although to a lesser extent. These findings are considered for their relevance to auditory theories of speech perception. © 1978 Psychonomic Society, Inc.},
keywords={auditory system;  central nervous system;  human cell;  normal human;  visual system, Adolescent;  Adult;  Female;  Human;  Lipreading;  Male;  Phonetics;  Speech Perception;  Visual Perception},
correspondence_address1={McGurk, H.; Department of Psychology, University of Surrey, Guildford, GU2 5XH, Surrey, United Kingdom},
issn={00315117},
coden={PEPSB},
pubmed_id={704285},
language={English},
abbrev_source_title={Percept. Psychophys.},
document_type={Article},
source={Scopus},
}

@article{LEZZOUM2016372,
title = "Echo threshold between passive and electro-acoustic transmission paths in digital hearing protection devices",
journal = "International Journal of Industrial Ergonomics",
volume = "53",
pages = "372 - 379",
year = "2016",
issn = "0169-8141",
doi = "https://doi.org/10.1016/j.ergon.2016.04.004",
url = "http://www.sciencedirect.com/science/article/pii/S0169814116300233",
author = "Narimene Lezzoum and Ghyslain Gagnon and Jérémie Voix",
keywords = "Hearing protection device, Echo threshold, Just noticeable difference, Delay",
abstract = "Electronic hearing protection devices are increasingly used in noisy environments. Theses devices feature a miniaturized external microphone and internal loudspeaker in addition to an analog or digital electronic circuit. They can transmit useful audio signals such as speech and warning signals to the protected ear and can reduce the sound pressure level using dynamic range compression. In the case of a digital electronic circuit, the transmission of audio signals may be noticeably delayed because of the latency introduced by the digital signal processor and by the analog-to-digital and digital-to-analog converters. These delayed audio signals will hence interfere with the audio signals perceived naturally through the passive acoustical path of the device. The proposed study presents an original procedure to evaluate, for two representative passive earplugs, the shortest delay at which human listeners start to perceive two sounds composed of the signal transmitted through the electronic circuit and the passively transmitted signal. This shortest delay is called the echo threshold and represents the delay between the time of perception of one fused sound from two separate sounds. In this study, a transient signal, a clean speech signal, a speech signal corrupted by factory noise, and a speech signal corrupted by babble noise are used to determine the echo thresholds of the two earplugs. Twenty untrained listeners participated in this study, and were asked to determine the echo thresholds using a test software in which attenuated signals are delayed from the original signals in real-time. The findings show that when using hearing devices, the echo threshold depends on four parameters: (a) the attenuation function of the device, (b) the duration of the signal, (c) the level of the background noise and (d) the type of background noise. Defined here as the shortest time delay at which at least 20% of the participants noticed an echo, the echo threshold was found to be 8 ms for a bell signal, 16 ms for clean speech and 22 ms for speech corrupted by babble noise when using a shallow earplug fit. When using a deep fit, the echo threshold was found to be 18 ms for a bell signal and 26 ms for clean speech and 68 ms for speech in factory. No echo threshold could be clearly determined for the speech signal in babble noise with a deep earplug fit."
}

@article{noel2017atypical,
  title={Atypical rapid audio-visual temporal recalibration in autism spectrum disorders},
  author={Noel, Jean-Paul and De Niear, Matthew A and Stevenson, Ryan and Alais, David and Wallace, Mark T},
  journal={Autism Research},
  volume={10},
  number={1},
  pages={121--129},
  year={2017},
  publisher={Wiley Online Library}
}

@article{stone2002tolerable,
  title={Tolerable hearing aid delays. II. Estimation of limits imposed during speech production},
  author={Stone, Michael A and Moore, Brian CJ},
  journal={Ear and Hearing},
  volume={23},
  number={4},
  pages={325--338},
  year={2002},
  publisher={LWW}
}

@article{bosker2020visual,
  title={How visual cues to speech rate influence speech perception},
  author={Bosker, Hans Rutger and Peeters, David and Holler, Judith},
  journal={Quarterly Journal of Experimental Psychology},
  pages={1747021820914564},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{article,
author = {Maier, Joost and Di Luca, Massimiliano and Noppeney, Uta},
year = {2011},
month = {02},
pages = {245-56},
title = {Audiovisual Asynchrony Detection in Human Speech},
volume = {37},
journal = {Journal of experimental psychology. Human perception and performance},
doi = {10.1037/a0019952}
}

@Article{soto2004assessing,
  author    = {Soto-Faraco, Salvador and Navarra, Jordi and Alsius, Agnes},
  journal   = {Cognition},
  title     = {Assessing automaticity in audiovisual speech integration: evidence from the speeded classification task},
  year      = {2004},
  number    = {3},
  pages     = {B13--B23},
  volume    = {92},
  comment   = {Nice easy and short, provides evidence for automaticity of multimodal integration via an indirect mcGurk Effect to induce perceived inconsistency, resulting in slower reaction times},
  publisher = {Elsevier},
}

@article{uslarente,
  title={Warum die Ente der Hund tadelt: M{\"o}gliche neue Wege in der Audiologe},
  author={Uslar, Verena Nicole and Carroll, Rebecca and Wendt, Dorothee and Ruigendijk, Esther and Branda, Thomas}
}

@Article{ross2007you,
  author    = {Ross, Lars A and Saint-Amour, Dave and Leavitt, Victoria M and Javitt, Daniel C and Foxe, John J},
  journal   = {Cerebral cortex},
  title     = {Do you see what I am saying? Exploring visual enhancement of speech comprehension in noisy environments},
  year      = {2007},
  number    = {5},
  pages     = {1147--1153},
  volume    = {17},
  publisher = {Oxford University Press},
}

@Book{stein1993merging,
  author    = {Stein, Barry E and Meredith, M Alex},
  publisher = {The MIT Press},
  title     = {The merging of the senses.},
  year      = {1993},
}

@Article{meredith1986visual,
  author    = {Meredith, M Alex and Stein, Barry E},
  journal   = {Journal of neurophysiology},
  title     = {Visual, auditory, and somatosensory convergence on cells in superior colliculus results in multisensory integration},
  year      = {1986},
  number    = {3},
  pages     = {640--662},
  volume    = {56},
  publisher = {American Physiological Society Bethesda, MD},
}

@InCollection{STEIN199379,
  author    = {Barry E. Stein and M. Alex Meredith and Mark T. Wallace},
  publisher = {Elsevier},
  title     = {Chapter 8 The visually responsive neuron and beyond: multisensory integration in cat and monkey},
  year      = {1993},
  editor    = {T.P. Hicks and S. Molotchnikoff and T. Ono},
  pages     = {79 - 90},
  series    = {Progress in Brain Research},
  volume    = {95},
  abstract  = {Publisher Summary
The cat superior colliculus neuron is a major site for the convergence and integration of multisensory information. It is only one of many central nervous system sites in many species where information from several modalities converges. Single cells in cat LS (lateral suprasylvian) and AES (anterior ectosylvian sulcus) and in monkey IPS (intraparietal sulcus) and STS (superior temporal sulcus) were examined. Although the spatial, temporal, and multiplicative characteristics of multisensory integration were most closely examined in cat cortex, all of the observations in monkey were consistent with those described in cat. The multisensory receptive fields of a single neuron overlapped one another in space, such that sensory stimuli that were in close spatial register fell within their excitatory receptive fields and enhanced the neuron's activity; spatially disparate stimuli produced either no interaction or depressed responses. The rules of multisensory integration evident at the level of the single neuron are also consistent with studies of intact behaving animals. The attentive and orientation responses cats make to visual and auditory stimuli were predictable based on the reactions of superior colliculus neurons to these stimuli. The data indicate that the visual responses of many neurons, whether in the superior colliculus or cortex, represent only one facet of their sensory coding capabilities.},
  doi       = {https://doi.org/10.1016/S0079-6123(08)60359-3},
  issn      = {0079-6123},
  url       = {http://www.sciencedirect.com/science/article/pii/S0079612308603593},
}

@Article{VANWASSENHOVE2007598,
  author   = {Virginie {van Wassenhove} and Ken W. Grant and David Poeppel},
  journal  = {Neuropsychologia},
  title    = {Temporal window of integration in auditory-visual speech perception},
  year     = {2007},
  issn     = {0028-3932},
  note     = {Advances in Multisensory Processes},
  number   = {3},
  pages    = {598 - 607},
  volume   = {45},
  abstract = {Forty-three normal hearing participants were tested in two experiments, which focused on temporal coincidence in auditory visual (AV) speech perception. In these experiments, audio recordings of/pa/and/ba/were dubbed onto video recordings of /ba/or/ga/, respectively (ApVk, AbVg), to produce the illusory “fusion” percepts /ta/, or /da/ [McGurk, H., & McDonald, J. (1976). Hearing lips and seeing voices. Nature, 264, 746–747]. In Experiment 1, an identification task using McGurk pairs with asynchronies ranging from −467ms (auditory lead) to +467ms was conducted. Fusion responses were prevalent over temporal asynchronies from −30ms to +170ms and more robust for audio lags. In Experiment 2, simultaneity judgments for incongruent and congruent audiovisual tokens (AdVd, AtVt) were collected. McGurk pairs were more readily judged as asynchronous than congruent pairs. Characteristics of the temporal window over which simultaneity and fusion responses were maximal were quite similar, suggesting the existence of a 200ms duration asymmetric bimodal temporal integration window.},
  doi      = {https://doi.org/10.1016/j.neuropsychologia.2006.01.001},
  keywords = {McGurk illusion, Multisensory, Time, Psychophysics, Analysis-by-synthesis},
  url      = {http://www.sciencedirect.com/science/article/pii/S002839320600011X},
}

@Misc{,
  author = {Audacity},
  title  = {Audacity® software is copyright © 1999-2020 Audacity Team.Web site: https://audacityteam.org/. It is free softwaredistributed under the terms of the GNU General Public License.The name Audacity® is a registered trademark of Dominic Mazzoni.},
}

@Manual{,
  title        = {RStudio: Integrated Development Environment for R},
  address      = {Boston, MA},
  author       = {{RStudio Team}},
  organization = {RStudio, PBC.},
  year         = {2020},
  url          = {http://www.rstudio.com/},
}

@Comment{jabref-meta: databaseType:bibtex;}
